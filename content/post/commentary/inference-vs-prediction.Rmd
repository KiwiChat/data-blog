---
title: "Inference vs Prediction"
lead: "Statistical modeling or machine learning?"
author: Matthias DÃ¶ring
draft: true
date: '2018-12-06'
description: "Inference is concerned with learning about the data generation process, while prediction is concerned with estimating the outcome for new observations. These contrasting principles originate from the statistical modeling and machine learning communities, respectively. Here, I try to showcase the differences and similarities between the two concepts."
slug: "inference-vs-prediction"
thumbnail: "/post/machine-learning/auc_performance.png"
categories:
  - commentary
tags:
    - R
---
The terms inference and prediction both describe tasks where we learn from data in a supervised manner in order to find a model that describes the relationship between the independent variables and the outcome. Inference and prediction, however, diverge when it comes to the question what one should do with the resulting model:

* **Inference:** Use the model to describe the data generation process and gain insights from this (e.g. inform decision making).
* **Prediction:** Use the model to predict the outcomes for new data points (e.g. as a black box).

Since the aims of inference and prediction are fundamentally different, the types of models that are used for inference and prediction, respectively, are often different. 

## Model interpretability is a necessity for inference

In essence, the difference between models that are suitable for inference and those that are not boilds down to model interpretability. What do I mean with *interpretable*? A model is called interpretable if it is possible to retrace how the model generates its estimates. Consider the following approaches for prediction:

* Interpretable: Generalized linear models (e.g. linear regression, logistic regression), linear discriminant analysis, linear support vector machines (SVMs), decision trees
* Less interpretable: neural networks, non-linear SVMs

Of these approaches, only a subset of the interpretable methods are useful for inference. For example, linear SVMs are interpretable because they provide a single coefficient for every feature such that it is possible to explain the impact of individual features on the prediction. However, SVMs are non-parametric because they do not assume that the data follow any specific distribution. Therefore, the uncertainty associated with the model coefficients (e.g. the variance) is not available and it is not possible to obtain an implicit measure of model confidence. Note that SVMs may output probabilities but these probabilities are just a transformation of the decision values and do not take into account the confidence about the estimated model parameters. Thus, SVMs are not suitable for inference. The same goes for decision trees.

In contrast, consider linear regression, which assumes that the data follow a Gaussian distribution. Among other things, [this allows us to find the standard error of the coefficient estimates and output confidence intervals](http://127.0.0.1:4321/post/machine-learning/linear_models/), which indicates our confidence about the parameter estimates as well as our predictions. Since linear regression allows us to understand the probabilistic nature of the data generation process, it is a suitable method for inference.

## Examples for prediction and inference

Note that, if you are using a model that is suitable for inference, this does not necessarily mean you are actually performing inference but it depends on what you are doing with the model. Consider the following example:

* **Prediction example:** You want to predict future ozone levels using historic data. Since you believe that there is a linear relationship between the ozone level and measurements of temperature, solar radiation, and wind, you fit several linear models (e.g. using different sets of features) on the training data and select the model minimizing the loss on the test set. Finally, you use the selected model for predicting the ozone level. Note that you do not care at all about the Gaussian assumption of the model or the additional information that is available on the model estimates because you are only interested in maximizing predictive performance.
* **Inference example:** You want to understand how ozone levels are influenced by temperature, solar radiation, and wind. Since you assume that the residuals are normally distributed, you use a linear regression model. Since you want to harness the information from the full data set and accuracy does not play a role, you fit the model on the full data set. On the basis of the fitted model, you interpret the role of the features on the ozone levels, for example, by considering the confidence bands of the estimates.

Note that [machine learning (predictive modeling) is usually concerned with prediction](https://projecteuclid.org/euclid.ss/1009213726), while statistics relies on stochastic models that perform inference. Therefore, the selection of machine learning models is based on the performance on the test set rather than the theoretical validity of the model. This is why [machine learning models are often treated as black boxes](https://xkcd.com/1838/) that can generate the correct answer as long as a sufficient number of data is provided. For inference, the tables are turned. Here, it is of the essence that the model reflects the actual data generation process as close as possible. To achieve this, prior knowledge is often incorporated into inference models.

## Workflows for inference and prediction

The basic workflows for inference and prediction are described in the following sections.

### Inference

1. Modeling: reason about the data generation process and choose the stocastic model that approximates the data generation process best.
2. Model validation: evaluate the validity of the stochastic model using residual analysis or goodness-of-fit tests.
3. Inference: use the stochastic model to understand the data generation process .

### Prediction

1. Modeling: consider several different models and different parameter settings.
2. Model selection: Identify the model with the greatest predictive performance using validation/test sets; select the model with the highest performance on the test set.
3. Prediction: Apply the selected model on new data with the expectation that the selected model also generalizes to the unseen data.

## Why are Bayesian models so popular for inference?

Bayesian method


## References
http://courses.csail.mit.edu/18.337/2015/docs/50YearsDataScience.pdf
Leo Breimann (creator of decision trees): statistical modeling: the two cultures
https://sahirbhatnagar.com/assets/pdf/stat_ml_jc_july20.pdf
https://xkcd.com/1838/
