---
title: "Interpreting Generalized Linear Models"
author: Matthias Döring
date: '2018-11-08T15:00:00Z'
draft: true
description: "TODO"
categories:
  - machine-learning
tags:
thumbnail: "/post/machine-learning/linear_models_cover.png"

---

Interpreting generalized linear models (GLM) obtained through ```glm``` is similar to [interpreting conventional linear models](/post/machine-learning/linear_models/). Still, there are some differences, which we will discuss here.

## Introduction to GLMs

GLMs enable the use of linear models in cases where the response variable has an error distribution that is non-normal. Each distribution is associated with a specific canonical link function. A link function $g(x)$ fulfills $X \beta = g(\mu)$. For example, for a Poisson distribution, the canonical link function is $g(\mu) = \text{ln}(\mu)$. This means that estimates on the original scale can be obtained by taking the inverse of the link function, in this case, the exponential function: $\mu = \exp(X \beta)$.

## Data preparation

We will take 70% of samples for training and 30% for testing:

```{r}
data(airquality)
ozone <- subset(na.omit(airquality), 
        select = c("Ozone", "Solar.R", "Wind", "Temp"))
set.seed(123)
N.train <- ceiling(0.7 * nrow(ozone))
N.test <- nrow(ozone) - N.train
trainset <- sample(seq_len(nrow(ozone)), N.train)
testset <- setdiff(seq_len(nrow(ozone)), trainset)
```

## Training a GLM

For investigating the characteristics of GLMs, we will train a model, which assumes that errors are Poisson distributed. 

By specifying ```family = "poisson"```, ```glm``` automatically selects the appropriate canonical link function, which is the logarithm. More information on possible families and their canonical link functions can be obtained via ```?family```.

```{r}
model.pois <- glm(Ozone ~ Solar.R + Temp + Wind, data = ozone, 
                family = "poisson", subset = trainset)
summary(model.pois)
```

In terms of the GLM output, there are four main aspects that are different to the output of ```lm```:

* Deviance (deviance of residuals / null deviance / residual deviance)
* Dispersion parameter
* AIC
* Fisher Scoring iterations

Let us investigate these aspects in more detail.

## Deviance residuals

We already know [residuals](/post/machine-learning/linear_models/) from the ```lm``` function. But what are deviance residuals? In ordinary least-squares, the residual associated with the $i$-th observation is defined as

\[r_i = y_i - \hat{f}(x_i)\]

where $\hat{f}(x) = \beta_0 + x^T \beta$ is the prediction function the fitted model.
For GLMs, there are [several ways for specifying residuals](
https://stackoverflow.com/questions/2531489/understanding-glmresiduals-and-residglm). To understand deviance residuals, it is worthwhile to look at the other types of residuals first. For this, we will first make the following definitions:

```{r}
expected <- ozone$Ozone[trainset]
g <- family(model.pois)$linkfun # log function
g.inv <- family(model.pois)$linkinv # exp function
estimates.log <- model.pois$linear.predictors # estimates on log scale
estimates <- fitted(model.pois) # estimates on response scale (exponentiated)
all.equal(g.inv(estimates.log), estimates)
```

We will discuss four types of residuals: response residuals, working residuals, Pearson residuals, and, finally, deviance residuals. There is also another type of residual called *partial residual*, which is formed by determining residuals from models where individual features are excluded. This residual is not discussed here.

### Response residuals

For ```type = "response"```, the *conventional* residual on the response level is computed, that is, 
\[r_i = y_i - \hat{f}(x_i)\]
This means that the fitted residuals are transformed by taking the inverse of the link function:

```{r}
# type = "response"
res.response1 <- residuals(model.pois, type = "response")
# note: fitted() returns the exponentiated estimates
res.response2 <- expected - estimates
all.equal(res.response1, res.response2)
```

### Working residuals

For ```type = "working"```, the *response residuals* are normalized by the estimates:

\[r_i = \frac{y_i - \hat{f}(x_i)}{\hat{f}(x_i)}\]


```{r}
# type = "working"
res.working1 <- residuals(model.pois, type="working")
res.working2 <- (expected - estimates) / estimates
all.equal(res.working1, res.working2)
```

### Pearson residuals

For ```type = "pearson"```, the Pearson residuals are computed. They are defined as
\[r_i = \frac{y_i - \hat{f}(x_i)}{\sqrt{\hat{f}(x_i)}}\]

```{r}
# type = "pearson"
res.pearson1 <- residuals(model.pois, type="pearson")
res.pearson2 <- (expected - estimates) / sqrt(estimates)
all.equal(res.pearson1, res.pearson2)
```

### Deviance residuals

Deviance residuals are defined by considering the deviance. The deviance of a model is defined by 

\[{D(y,{\hat {\mu }})=2{\Big (}\log {\big (}p(y\mid {\hat {\theta }}_{s}){\big )}-\log {\big (}p(y\mid {\hat {\theta }}_{0}){\big )}{\Big )}.\,}\]

where

* $y$ is the outcome
* $\hat{\mu}$ is the estimate of the model
* $\hat{\theta}_s$ and $\hat{\theta}_0$ are the parameters of the fitted *saturated* and *proposed models*, respectively
* $p(y | \theta)$ is the likelihood of data given the model

So, what is a saturated model? A saturated model has as many parameters as it has training points, that is, $p = n$. The proposed model can be any other type of model. For the residual deviance, this is the model that you have trained.

This means that the deviance indicates the extent to which the likelihood of the saturated model exceeds the likelihood of the proposed model. If the proposed model has a good fit, the deviance will be small. If the proposed model has a bad fit, the deviance will be high. For example, for the Poisson model, the deviance is

\[D = 2 \cdot \sum_{i = 1}^n y_i \cdot \log \left(\frac{y_i}{\hat{\mu}_i}\right) − (y_i − \hat{\mu}_i)\,.\]

In R, the deviance residuals represent the contributions of individual samples to the deviance $D$. More specifically, they are defined as the signed [square roots of the unit deviances](https://www.youtube.com/watch?v=JC56jS2gVUE). Thus, the deviance residuals are analogous to the conventional residuals: when they are squared, we obtain the sum of squares that we use for assessing the fit of the model. However, while the sum of squares is the residual sum of
squares for linear models, for GLMs, this is the deviance. 

How does such a deviance look like in practice? For example, for the Poisson distribution, the deviance residuals are defined as:

\[r_i = \text{sgn}(y - \hat{\mu}_i) \cdot \sqrt{2 \cdot y_i \cdot \log \left(\frac{y_i}{\hat{\mu}_i}\right) − (y_i − \hat{\mu}_i)}\,.\]

Let us verify this in R:

```{r}
# type = "deviance"
res.dev1 <- residuals(model.pois, type = "deviance")
res.dev2 <- residuals(model.pois)
poisson.dev <- function (y, mu) 
    # unit deviance
    2 * (y * log(ifelse(y == 0, 1, y/mu)) - (y - mu))
res.dev3 <- sqrt(poisson.dev(expected, estimates)) * 
        ifelse(expected > estimates, 1, -1)
all.equal(res.dev1, res.dev2, res.dev3)
```

Note that, for ordinary least-squares models, the [deviance residual is identical to the conventional residual](http://people.stat.sfu.ca/~raltman/stat402/402L11.pdf).

Now, how do these values look for our model?

```{r}
summary(residuals(model.pois))
```

Since the median deviance residual is close to zero, this means that our model is not biased in one direction (i.e. the out come is neither over- nor underestimated). 

## Null and residual deviance

Since we have already introduced the deviance, [understanding the null and residual deviance](https://stats.stackexchange.com/questions/108995/interpreting-residual-and-null-deviance-in-glm-r) is not a challenge anymore. Remember the definition of the deviance?

\[{D(y,{\hat {\mu }})=2{\Big (}\log {\big (}p(y\mid {\hat {\theta }}_{s}){\big )}-\log {\big (}p(y\mid {\hat {\theta }}_{0}){\big )}{\Big )}.\,}\]

If, $\theta_0$ refers to the null model (i.e. an intercept-only model), this gives rise to the *null deviance*. 
If, however, $\theta_0$ refers to the trained model, this gives rise to the *residual deviance*. So, how can we interpret these two quantities?

* Null deviance: A low null deviance implies that the data can be modeled well merely using the intercept. If the null deviance is low, you should consider using few features for modeling the data.
* Residual deviance: A low residual deviance implies that the model you have trained is appropriate. Congratulations!

How does this look for our model?

```{r}
paste0(c("Null deviance: ", "Residual deviance: "),
       round(c(model.pois$null.deviance, deviance(model.pois)), 2))
```

These results are somehow reassuring. First, the null deviance is high, which means it makes sense to use more than a single parameter. Second, the residual deviance is relatively low, which indicates that the log likelihood of our model is close to the log likelihood of the saturated model. However, for a well-fitting model, the residual deviance should be close to the degrees of freedom (74), which is not the case. For example, this could be a result of
overdispersion where the variation is greater than predicted by the model. This can happen for a Poisson model, if the actual variance exceeds the mean, although $Var(Y) = \mu$ is assumed by the model.

## Dispersion parameter

To understand the dispersion parameter, we have to understand the concept of dispersion first. Dispersion (variability/scatter/spread) simply indicates whether a distribution is wide or narrow.

For likelihood-based model, the dispersion parameter is always fixed to 1. It is adjusted only for methods that are based on quasi-likelihood estimation such as ```quasipoisson``` or ```quasibinomial```. These methods can deal with overdispersion.

## AIC

## Fisher scoring iterations
