---
title: "An Introduction to Probabilistic Programming with Stan in R"
author: Matthias Döring
date: '2018-11-26'
description: "Bayesian modeling does not have to be tedious. Using probabilistic programming it is relatively easy to implement statistical models that make use of MCMC sampling. In this post, I explore probabilistic programming using Stan."
thumbnail: "/post/machine-learning/probabilistic_programming_cover.png"
categories:
  - machine-learning
tags:
    - Bayesian
    - R
---



<p>Probabilistic programming enables us to implement statistical models without having to worry about the technical details. It is particularly useful for Bayesian models that are based on MCMC sampling. In this article, I investigate how <a href="http://mc-stan.org/">Stan</a> can be used through its implementation in R, <a href="http://mc-stan.org/rstan/">RStan</a>. This post is largely based on the <a href="https://github.com/stan-dev/rstan/wiki/RStan-Getting-Started">GitHub documentation of Rstan</a> and <a href="http://mc-stan.org/rstan/articles/rstan.html">its vignette</a>.</p>
<div id="introduction-to-stan" class="section level2">
<h2>Introduction to Stan</h2>
<p>Stan is a C++ library for Bayesian inference. It is based on the No-U-Turn sampler (NUTS), which is used for estimating the posterior distribution according to a user-specified model and data. Performing an analysis using Stan involves the following steps:</p>
<ol style="list-style-type: decimal">
<li>Specify the statistical model using the the Stan modeling language. This is typically done through a dedicated <em>.stan</em> file.</li>
<li>Prepare the data that is to be fed to the model.</li>
<li>Sample from the posterior distribution using the <code>stan</code> function.</li>
<li>Analyze the results.</li>
</ol>
<p>In this article, I will showcase the use of Stan using two hierarchical models. I will use the first model to talk about the basic features of Stan and the second example to demonstrate a more advanced application.</p>
<div id="the-eight-schools-data-set" class="section level3">
<h3>The eight schools data set</h3>
<p>The first data set we are going to use is the <a href="https://andrewgelman.com/2014/01/21/everything-need-know-bayesian-statistics-learned-eight-schools/">eight schools data set</a> (Rubin, 1981 and Gelman, 2003). This data set measures the effect of coaching programs on college admission tests, the scholastic aptitude test (SAT), which is used in the US. The SAT was designed in such a way that it should be resistant to short-term efforts directly targeted at improving the score in the test. Rather, the test should reflect the knowledge that was acquired over a longer period of time. The data set looks as follows:</p>
<table style="width:78%;">
<colgroup>
<col width="16%" />
<col width="25%" />
<col width="36%" />
</colgroup>
<thead>
<tr class="header">
<th>School</th>
<th>Estimated effect of coaching (<span class="math inline">\(y_j\)</span>)</th>
<th>Standard error of effect (<span class="math inline">\(\sigma_j\)</span>)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>A</td>
<td>28</td>
<td>15</td>
</tr>
<tr class="even">
<td>B</td>
<td>8</td>
<td>10</td>
</tr>
<tr class="odd">
<td>C</td>
<td>-3</td>
<td>16</td>
</tr>
<tr class="even">
<td>D</td>
<td>7</td>
<td>11</td>
</tr>
<tr class="odd">
<td>E</td>
<td>-1</td>
<td>9</td>
</tr>
<tr class="even">
<td>F</td>
<td>1</td>
<td>11</td>
</tr>
<tr class="odd">
<td>G</td>
<td>18</td>
<td>10</td>
</tr>
<tr class="even">
<td>H</td>
<td>12</td>
<td>18</td>
</tr>
</tbody>
</table>
<p>As we can see, the SAT did not fulfill its promise: for most of the eight schools, the short-term coaching indeed increased the SAT scores as evidenced by positive values of <span class="math inline">\(y_j\)</span>, where <span class="math inline">\(y_j\)</span> indicates the change in SAT scores. For this data set, we are interested in estimating the true effect size associated with each school. There are two alternative approaches that could be used. First, we could assume that all schools are independent of each other. However, this would lead to estimates that are hard to interpret because the 95% posterior intervals for the schools would largely overlap due to the high standard error. Second, one could pool the data from all schools, assuming that the true effect is the same in all schools. This, however, is also not reasonable because there should be school-specific effects (e.g. different teachers and students).</p>
<p>Thus, another model is required. The hierarchical model has the advantage of combining information from all eight schools (<em>experiments</em>) without assuming that they have common true effects. We can specify a hierarchial Bayesian model in the following way:</p>
<p><span class="math display">\[
\begin{align}
y_i &amp;\sim \text{Normal}(\theta_j, \sigma_j)\,, j = 1, \ldots, 8 &amp; \text{Prior distribution}\\
\theta_j &amp;\sim \text{Normal}(\mu, \tau)\,, j = 1, \ldots, 8 &amp; \text{Posterior distribution} \\
p(\mu, \tau) &amp;\propto 1 &amp; \text{Parameter distribution is uniform} 
\end{align}
\]</span></p>
<p>According to the model, the effects of coaching follow a normal distribution whose mean is the true effect, <span class="math inline">\(\theta_j\)</span>, and whose standard deviation is <span class="math inline">\(\sigma_j\)</span>, which is known from the data. The true effect, <span class="math inline">\(\theta_j\)</span>, follows a normal distribution with parameters <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\tau\)</span>.</p>
</div>
<div id="defining-the-stan-model-file" class="section level3">
<h3>Defining the Stan model file</h3>
<p>Having specified the model we are going to use, we can now deliberate about how to specify this model in Stan. Before defining the Stan program for the model specified above, let us take a look at the structure of the Stan modeling language.</p>
<div id="variables" class="section level4">
<h4>Variables</h4>
<p>In Stan, variables can be defined in the following way:</p>
<pre><code>int&lt;lower=0&gt; n; # lower bound is 0
int&lt;upper=5&gt; n; # upper bound is 5
int&lt;lower=0,upper=5&gt; n; # n is in [0,5]</code></pre>
<p>Note that the upper and lower boundaries of the variables should be specified, if they are known a priori.</p>
<p>Multi-dimensional data can be specified via square brackets:</p>
<pre><code>vector[n] numbers; // a vector of length n
real[n] numbers;  // an array of floats with length n
matrix[n,n] matrix; // an n times n matrix</code></pre>
</div>
<div id="program-blocks" class="section level4">
<h4>Program blocks</h4>
<p>The following program blocks are used in Stan:</p>
<ul>
<li><em>data</em>: for specifying the data that is conditioned upon using Bayes rule</li>
<li><em>transformed data</em>: for preprocessing the data</li>
<li><em>parameters</em> (required): for specifying the parameters of the model</li>
<li><em>transformed parameters</em>: for parameter processing before computing the posterior</li>
<li><em>model</em> (required): for specifying the model itself</li>
<li><em>generated quantities</em>: for postprocessing the results</li>
</ul>
<p>For the <em>model</em> program block, distributions can be specified in two equivalent ways. The first one, uses the following statistical notation:</p>
<pre><code>y ~ normal(mu, sigma); # y follows a normal distribution</code></pre>
<p>The second way uses a programmatic notation based on the log probability density function (lpdf):</p>
<pre><code>target += normal_lpdf(y | mu, sigma); # increment the normal log density</code></pre>
<p>Stan supports a large number of probability distributions. When specifying models via Stan, the <code>lookup</code> function comes in handy: It provides a mapping from R functions to Stan functions. Consider the following example:</p>
<pre class="r"><code>library(rstan) # load stan package
lookup(rnorm)</code></pre>
<pre><code>##     StanFunction             Arguments ReturnType Page
## 355   normal_rng (real mu, real sigma)       real  494</code></pre>
<p>Here, we see that the equivalent of <code>rnorm</code> in R is <code>normal_rng</code> in Stan.</p>
</div>
<div id="the-eight-schools-model" class="section level4">
<h4>The eight schools model</h4>
<p>Now that we know the basics of the Stan modeling langeu, we can define our model, which we will store in a file called <code>schools.stan</code>:</p>
<pre><code>data {
  int&lt;lower=0&gt; n; //number of schools
  real y[n]; // effect of coaching
  real&lt;lower=0&gt; sigma[n]; // standard errors of effects
}
parameters {
  real mu;  // the overall mean effect
  real&lt;lower=0&gt; tau; // the inverse variance of the effect
  vector[n] eta; // standardized school-level effects (see below)
}
transformed parameters {
  vector[n] theta; 
  theta = mu + tau * eta; // find theta from mu, tau, and eta
}
model {
  target += normal_lpdf(eta | 0, 1); // eta follows standard normal
  target += normal_lpdf(y | theta, sigma);  // y follows normal with mean theta and sd sigma
}</code></pre>
<p>Note that <span class="math inline">\(\theta\)</span> never appears in the parameters. This is because we do not explicitly model <span class="math inline">\(\theta\)</span> but instead model <span class="math inline">\(\eta\)</span>, the standardized effect for individual schools. We then construct <span class="math inline">\(\theta\)</span> in the <em>transformed parameters</em> section according to <span class="math inline">\(\mu\)</span>, <span class="math inline">\(\tau\)</span>, and <span class="math inline">\(\eta\)</span>. This parameterization makes the sampler more efficient.</p>
<p>Note that the model is specified using vector notation since both <span class="math inline">\(\theta\)</span> and <span class="math inline">\(\sigma\)</span> indicate vectors. This allows for improved runtimes because it is not necessary to loop over every individual element.</p>
</div>
</div>
<div id="preparing-the-data-for-modeling" class="section level3">
<h3>Preparing the data for modeling</h3>
<p>Before we can fit the model, we need to encode the input data as a list whose parameters should correspond to the entries in the data section of the Stan model. For the schools data, the data are the following:</p>
<pre class="r"><code>schools.data &lt;- list(
  n = 8,
  y = c(28,  8, -3,  7, -1,  1, 18, 12),
  sigma = c(15, 10, 16, 11,  9, 11, 10, 18)
)</code></pre>
</div>
<div id="sampling-from-the-posterior-distribution" class="section level3">
<h3>Sampling from the posterior distribution</h3>
<p>We can sample from the posterior distribution using the <code>stan</code> function, which performs the following three steps:</p>
<ol style="list-style-type: decimal">
<li>It translate the model specification to C++ code.</li>
<li>It compiles the C++ code to a shared object.</li>
<li>It samples from the posterior distribution according to the specified model, data, and settings.</li>
</ol>
<p>If <code>rstan_options(auto_write = TRUE)</code> has been executed in the active R session, subsequent calls of the same model will be much faster than the first call because the <code>stan</code> function then skips the first two steps (translating and compiling the model). Additionally, we will set the number of cores to be used:</p>
<pre class="r"><code>options(mc.cores = parallel::detectCores()) # parallelize
rstan_options(auto_write = TRUE)  # store compiled stan model</code></pre>
<p>Now, we can compile the model and sample from the posterior. The only two required parameters of <code>stan</code> are the location of the model file and the data to be fed to the model. The other parameters indicated here are just shown to provide an overview of the possible options:</p>
<pre class="r"><code>fit1 &lt;- stan(
  file = &quot;schools.stan&quot;,  # Stan program
  data = schools.data,    # named list of data
  chains = 4,             # number of Markov chains
  warmup = 1000,          # number of warmup iterations per chain
  iter = 2000,            # total number of iterations per chain
  refresh = 1000          # show progress every &#39;refresh&#39; iterations
  )</code></pre>
</div>
<div id="model-interpretation" class="section level3">
<h3>Model interpretation</h3>
<p>We will first do a basic interpretation of the model and then investigate the MCMC procedure.</p>
<div id="basic-model-interpretation" class="section level4">
<h4>Basic model interpretation</h4>
<p>To perform inference using the fitted model, we can use the <code>print</code> function.</p>
<pre class="r"><code>print(fit1) # optional parameters: pars, probs</code></pre>
<pre><code>## Inference for Stan model: schools.
## 4 chains, each with iter=2000; warmup=1000; thin=1; 
## post-warmup draws per chain=1000, total post-warmup draws=4000.
## 
##            mean se_mean   sd   2.5%    25%    50%    75%  97.5% n_eff Rhat
## mu         7.93    0.14 5.27  -2.20   4.51   7.90  11.24  18.46  1350 1.00
## tau        6.51    0.17 5.78   0.23   2.48   5.11   9.08  20.21  1115 1.00
## eta[1]     0.38    0.02 0.93  -1.49  -0.21   0.41   1.01   2.17  3538 1.00
## eta[2]     0.02    0.02 0.87  -1.69  -0.56   0.00   0.58   1.85  2917 1.00
## eta[3]    -0.21    0.02 0.94  -2.04  -0.83  -0.19   0.44   1.61  3469 1.00
## eta[4]    -0.03    0.02 0.90  -1.81  -0.62  -0.02   0.57   1.75  3071 1.00
## eta[5]    -0.37    0.01 0.87  -2.00  -0.94  -0.39   0.20   1.38  3398 1.00
## eta[6]    -0.23    0.02 0.90  -1.95  -0.83  -0.24   0.36   1.58  2787 1.00
## eta[7]     0.34    0.02 0.91  -1.46  -0.23   0.36   0.92   2.09  3202 1.00
## eta[8]     0.06    0.02 0.96  -1.86  -0.58   0.08   0.72   1.96  4000 1.00
## theta[1]  11.19    0.16 8.13  -2.31   5.76  10.26  15.23  30.61  2477 1.00
## theta[2]   7.94    0.10 6.33  -4.78   4.04   7.90  11.89  20.58  3682 1.00
## theta[3]   6.17    0.16 7.75 -11.90   1.88   6.71  11.07  19.91  2365 1.00
## theta[4]   7.68    0.10 6.61  -5.55   3.60   7.61  11.60  21.47  4000 1.00
## theta[5]   5.15    0.11 6.40  -9.14   1.38   5.49   9.44  16.80  3209 1.00
## theta[6]   6.12    0.12 6.73  -8.17   1.99   6.49  10.48  18.53  3005 1.00
## theta[7]  10.66    0.14 6.86  -1.06   6.10   9.94  14.68  26.66  2509 1.00
## theta[8]   8.48    0.14 7.70  -7.57   4.08   8.26  12.70  24.83  3094 1.00
## lp__     -39.63    0.08 2.70 -45.75 -41.23 -39.40 -37.72 -35.13  1085 1.01
## 
## Samples were drawn using NUTS(diag_e) at Thu Nov 29 10:28:10 2018.
## For each parameter, n_eff is a crude measure of effective sample size,
## and Rhat is the potential scale reduction factor on split chains (at 
## convergence, Rhat=1).</code></pre>
<p>Here, the row names indicate the estimated parameters: mu is the mean of the posterior distribution and tau is its standard deviation. The entries for eta and theta indicate the estimates for the vectors <span class="math inline">\(\eta\)</span> and <span class="math inline">\(\theta\)</span>, respectively. The <em>lp</em> entry shows the log density, which is typically not relevant. The columns indicate the computed values. The percentages indicate the credible intervals. For example, the 95% credible interval for the overall effect of coaching, <span class="math inline">\(\mu\)</span>, is <span class="math inline">\([-1.27, 18.26]\)</span>. Since we are not very certain of the mean, the 95% credible intervals for <span class="math inline">\(\theta_j\)</span> are also quite wide. For example, for the first school, the 95% credible interval is <span class="math inline">\([-2.19, 32.33]\)</span>.</p>
<p>We can visualize the uncertainty in the estimates using the <code>plot</code> function:</p>
<pre class="r"><code># specify the params to plot via pars
plot(fit1, pars = &quot;theta&quot;)</code></pre>
<p><img src="/post/machine-learning/probabilistic_programming_files/figure-html/unnamed-chunk-6-1.png" width="672" /></p>
<p>The black lines indicate the 95% intervals, while the red lines indicate the 80% intervals. The circles indicate the estimate of the mean.</p>
<p>We can obtain the generated samples using the <code>extract</code> function:</p>
<pre class="r"><code># retrieve the samples
samples &lt;- extract(fit1, permuted = TRUE) # 1000 samples per parameter
mu &lt;- samples$mu  # samples of mu only</code></pre>
</div>
<div id="mcmc-diagnostics" class="section level4">
<h4>MCMC diagnostics</h4>
<p>There are also functions for diagnosing the <a href="https://jeremykun.com/2015/04/06/markov-chain-monte-carlo-without-all-the-bullshit/">MCMC procedure</a>. By plotting the trace of the sampling procedure, we can identify whether anything has gone wrong during sampling. This could for example be the case if the chain stays in one place for too long or makes too many steps in one direction. We can plot the traces of the four chains used in our model with the <code>traceplot</code> function:</p>
<pre class="r"><code># diagnostics:
traceplot(fit1, pars = c(&quot;mu&quot;, &quot;tau&quot;), inc_warmup = TRUE, nrow = 2)</code></pre>
<p><img src="/post/machine-learning/probabilistic_programming_files/figure-html/unnamed-chunk-8-1.png" width="672" /></p>
<p>Both traces look fine to me.</p>
<p>To obtain the samples from individual Markov chains, we can use the <code>extract</code> function once again:</p>
<pre class="r"><code># retrieve matrix of iterations, chains, and parameters
chain.data &lt;- extract(fit1, permuted = FALSE) 
print(chain.data[1,,]) # parameters of all chains for the 1st of 1000 iterations</code></pre>
<pre><code>##          parameters
## chains           mu      tau      eta[1]     eta[2]     eta[3]      eta[4]
##   chain:1 -2.746330 4.331718 -0.09236649  0.3273003 -0.8243994  2.00034759
##   chain:2 10.524676 7.498804  0.61707889 -0.5950907 -0.8951201 -0.44002480
##   chain:3  6.830166 3.441065  1.38809721 -0.4728165  1.4512123  0.09198391
##   chain:4 -1.027019 5.399081  0.57620713 -0.2192507 -1.2519190  1.75575194
##          parameters
## chains       eta[5]     eta[6]     eta[7]     eta[8]  theta[1]  theta[2]
##   chain:1 -0.422475 -1.2525170  1.5072839  1.3500444 -3.146436 -1.328557
##   chain:2 -1.724786 -0.4515949  0.9911818  2.0868215 15.152030  6.062207
##   chain:3  1.877451 -0.5970420 -1.1753330 -0.7597141 11.606699  5.203173
##   chain:4  1.231959  0.1916635  1.3099377  0.3222195  2.083970 -2.210771
##          parameters
## chains     theta[3] theta[4]  theta[5]     theta[6]  theta[7]   theta[8]
##   chain:1 -6.317396 5.918612 -4.576373 -8.171880650  3.782799  3.1016817
##   chain:2  3.812345 7.225016 -2.409155  7.138253954 17.957354 26.1733422
##   chain:3 11.823882 7.146688 13.290598  4.775705034  2.785768  4.2159400
##   chain:4 -7.786231 8.452428  5.624427  0.007787911  6.045441  0.7126701
##          parameters
## chains         lp__
##   chain:1 -42.70212
##   chain:2 -38.73482
##   chain:3 -42.12478
##   chain:4 -40.44153</code></pre>
<p>To perform more advanced analytics of the sampling process, we can use the <code>shinystan</code> package, which provides a Shiny frontend. Using the package, a fitted model can be analyzed by starting the Shiny application in the following way:</p>
<pre class="r"><code>library(shinystan)
launch_shinystan(fit1)</code></pre>
</div>
</div>
</div>
<div id="hierarchical-regression" class="section level2">
<h2>Hierarchical regression</h2>
<p>Now that we have a basic understanding of Stan, we can dive into a more advanced application: Let’s try our hands at hierarchical regression. In conventional regression, we model a relationship of the form</p>
<p><span class="math display">\[Y = \beta_0 + X \beta\,.\]</span></p>
<p>This representation assumes that all samples have the same distribution. If there is a grouping of samples, then we have a problem because latent differences within and between groups will be ignored.</p>
<p>An alternative would be to have one regression model for each group. In this case, however, the small sample size would be problematic when estimating the individual models.</p>
<p>Hierarchical regression is a compromise between both extremes. This model assumes that the groups are similar but yet exhibit differences.</p>
<p>Assume that each sample belongs to one of <span class="math inline">\(K\)</span> groups. Then, hierarchical regression is specified as follows:</p>
<p><span class="math display">\[Y_k = \alpha_{k} + X_k \beta^{(k)}\,, \forall k \in \{1, \ldots, K\} \]</span></p>
<p>where <span class="math inline">\(Y_k\)</span> is the outcome for the <span class="math inline">\(k\)</span>-th group, <span class="math inline">\(\alpha_{k}\)</span> is the intercept, <span class="math inline">\(X_k\)</span> are the features, and <span class="math inline">\(\beta^{(k)}\)</span> indicates the weights. The hierarchical model is different from a model where <span class="math inline">\(Y_k\)</span> is fit for each group individually because the parameters, <span class="math inline">\(\alpha_{k}\)</span> and <span class="math inline">\(\beta^{(k)}\)</span> are assumed to originate from a common distribution.</p>
<div id="the-rats-data-set" class="section level3">
<h3>The rats data set</h3>
<p>A classic example for hierarchical regression is the rats data set. This longitudinal data set contains the weights of rats as measured for 5 weeks. Let us load the data:</p>
<pre class="r"><code>library(RCurl)
# load data as character
f &lt;- getURL(&#39;https://www.datascienceblog.net/data-sets/rats.txt&#39;)
# read table from text connection
df &lt;- read.delim(textConnection(f), header=T, sep = &quot; &quot;)
print(head(df)) # each row corresponds to an individual rat</code></pre>
<pre><code>##   day8 day15 day22 day29 day36
## 1  151   199   246   283   320
## 2  145   199   249   293   354
## 3  147   214   263   312   328
## 4  155   200   237   272   297
## 5  135   188   230   280   323
## 6  159   210   252   298   331</code></pre>
<p>Let us investigate the data:</p>
<pre class="r"><code>library(reshape2)
ddf &lt;- cbind(melt(df), Group = rep(paste0(&quot;Rat&quot;, seq(1, nrow(df))), 5))
library(ggplot2)
ggplot(ddf, aes(x = variable, y = value, group = Group)) + geom_line() + geom_point()</code></pre>
<p><img src="/post/machine-learning/probabilistic_programming_files/figure-html/unnamed-chunk-12-1.png" width="672" /></p>
<p>The data show a linear growth trend that is quite similar for different rats. However, we also see that the rats have different initial weights, which require different intercepts, as well as different growth rates, which require different slopes. Thus, a hierarchical model seems appropriate.</p>
</div>
<div id="specification-of-the-hierarchical-regression-model" class="section level3">
<h3>Specification of the hierarchical regression model</h3>
<p>The model can be specified as follows:</p>
<p><span class="math display">\[
\begin{align*}
Y_{ij} &amp;\sim \text{Normal} (\alpha_i + \beta_i (x_j - \bar{x}), \sigma_Y) \\ 
\alpha_i &amp;\sim \text{Normal}(\mu_{\alpha}, \sigma_{\alpha}) \\
\beta_i &amp;\sim \text{Normal}(\mu_{\beta}, \sigma_{\beta}) \\
\end{align*}
\]</span></p>
<p>where the intercept for the <span class="math inline">\(i\)</span>-th rat is indicated by <span class="math inline">\(\alpha_i\)</span> and the slope by <span class="math inline">\(\beta_i\)</span>. Note that the measurement times are centered around <span class="math inline">\(\bar{x} = 22\)</span> is the median measurement (day 22) of the time-series data.</p>
<p>We can now specify the model and store it in a file called <code>rats.stan</code>:</p>
<pre><code>data {
    int&lt;lower=0&gt; N; // the number of rats
    int&lt;lower=0&gt; T; // the number of time points
    real x[T]; // day at which measurement was taken
    real y[N,T]; // matrix of weight times time
    real xbar; // the median number of days in the time series
}
parameters {
  real alpha[N]; // the intercepts of rat weights
  real beta[N]; // the slopes of rat weights

  real mu_alpha; // the mean intercept
  real mu_beta; // the mean slope

  real&lt;lower=0&gt; sigmasq_y;
  real&lt;lower=0&gt; sigmasq_alpha;
  real&lt;lower=0&gt; sigmasq_beta;
}
transformed parameters {
  real&lt;lower=0&gt; sigma_y; // sd of rat weight
  real&lt;lower=0&gt; sigma_alpha; // sd of intercept distribution
  real&lt;lower=0&gt; sigma_beta; // sd of slope distribution

  sigma_y &lt;- sqrt(sigmasq_y);
  sigma_alpha &lt;- sqrt(sigmasq_alpha);
  sigma_beta &lt;- sqrt(sigmasq_beta);
}
model {
  mu_alpha ~ normal(0, 100); // non-informative prior
  mu_beta ~ normal(0, 100); // non-informative prior
  sigmasq_y ~ inv_gamma(0.001, 0.001); // conjugate prior of normal
  sigmasq_alpha ~ inv_gamma(0.001, 0.001); // conjugate prior of normal
  sigmasq_beta ~ inv_gamma(0.001, 0.001); // conjugate prior of normal
  alpha ~ normal(mu_alpha, sigma_alpha); // all intercepts are normal 
  beta ~ normal(mu_beta, sigma_beta);  // all slopes are normal
  for (n in 1:N) // for each sample
    for (t in 1:T)  // for each time point
      y[n,t] ~ normal(alpha[n] + beta[n] * (x[t] - xbar), sigma_y);

}
generated quantities {
  // determine the intercept at time 0 (birth weight)
  real alpha0;
  alpha0 &lt;- mu_alpha - xbar * mu_beta;
}
</code></pre>
<p>Note that the model code estimates the variance (the <em>sigmasq</em> variables) rather than the standard deviations. Additionally, the <em>generated quantities</em> block explicitly calculates <span class="math inline">\(\alpha_0\)</span>, the intercept at time 0, that is, the weight of the rats at birth. We could have also calculated any other quantity in the <em>generated quantities</em> block, for example, the estimated weight of the rats at different points in time. We will do this in R later.</p>
</div>
<div id="data-preparation" class="section level3">
<h3>Data preparation</h3>
<p>To prepare the data for the model, we first extract the measurement points as numeric values and then encode everything in a list structure:</p>
<pre class="r"><code>days &lt;- as.numeric(regmatches(colnames(df), regexpr(&quot;[0-9]*$&quot;, colnames(df))))
rat.data &lt;- list(N = nrow(df), T = ncol(df), x = days,
                 y = df, xbar = median(days)) </code></pre>
</div>
<div id="fitting-the-regression-model" class="section level3">
<h3>Fitting the regression model</h3>
<p>We can now fit the Bayesian hierarchical regression model for the rat weight data set:</p>
<pre class="r"><code>rat.model &lt;- stan(
  file = &quot;rats.stan&quot;,
  data = rat.data)
# model contains estimates for intercepts (alpha) and slopes (beta)</code></pre>
</div>
<div id="prediction-with-the-hiearchical-regression-model" class="section level3">
<h3>Prediction with the hiearchical regression model</h3>
<p>Having determined <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span> for each rat, we can now estimate the weight of individual rats at arbitrary points in time. Here, we are interested in finding the weight of the rats from day 0 to day 100.</p>
<pre class="r"><code>predict.rat.weight &lt;- function(rat.model, newdays) {
    # newdays: vector of time points to consider
    rat.fit &lt;- extract(rat.model)
    alpha &lt;- rat.fit$alpha
    beta &lt;- rat.fit$beta
    xbar &lt;- 22 # hardcoded since not stored in rat.model
    y &lt;- lapply(newdays, function(t) alpha + beta * (t - 22))
    return(y)
}
newdays &lt;- seq(0, 100)
pred.weights &lt;- predict.rat.weight(rat.model, newdays)
# extract means and standard deviations from posterior samples
pred.means &lt;- lapply(pred.weights, function(x) apply(x, 2, mean))
pred.sd &lt;- lapply(pred.weights, function(x) apply(x, 2, sd)) 
# create plotting data frame with 95% CI interval from sd
pred.df &lt;- data.frame(Weight = unlist(pred.means), 
              Upr_Weight = unlist(pred.means) + 1.96 * unlist(pred.sd), 
              Lwr_Weight = unlist(pred.means) - 1.96 * unlist(pred.sd), 
              Day = unlist(lapply(newdays, function(x) rep(x, 30))),
              Rat = rep(seq(1,30), length(newdays)))
# predicted mean weight of all rats
ggplot(pred.df, aes(x = Day, y = Weight, group = Rat)) +
    geom_line()</code></pre>
<p><img src="/post/machine-learning/probabilistic_programming_files/figure-html/unnamed-chunk-15-1.png" width="672" /></p>
<pre class="r"><code># predictions for selected rats
sel.rats &lt;- c(9, 8, 29)
ggplot(pred.df[pred.df$Rat %in% sel.rats, ], 
       aes(x = Day, y = Weight, group = Rat, 
           ymin = Lwr_Weight, ymax = Upr_Weight)) +   
    geom_line()  +
    geom_errorbar(width=0.2, size=0.5, color=&quot;blue&quot;)</code></pre>
<p><img src="/post/machine-learning/probabilistic_programming_files/figure-html/unnamed-chunk-15-2.png" width="672" /></p>
<p>In contrast to the original data, the estimates from the model are smooth because each curve follows a linear model. Investigating the confidence intervals shown in the last plot, we can see that the variance estimates are reasonable. We are confident about the rat weights at the time of sampling (days 8 to 36) but the uncertainty increases the further we move away from the sampled region.</p>
</div>
</div>
<div id="where-to-go-from-here" class="section level2">
<h2>Where to go from here?</h2>
<p>I am really impressed that Bayesian approaches have become so accessible through probabilistic programming. There are <a href="https://github.com/stan-dev/example-models">dozens of Stan usage examples available at GitHub</a>. If you would like to learn more about Bayesian models, I would <a href="http://www.stat.columbia.edu/~gelman/book/">recommend the book <em>Bayesian Data Analysis</em> from Andrew Gelman</a>.</p>
</div>
