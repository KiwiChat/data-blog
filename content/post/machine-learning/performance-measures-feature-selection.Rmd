---
title: "Performance Measures for Feature Selection"
author: Matthias DÃ¶ring
date: '2018-11-20'
draft: true
description: "TODO"
categories:
  - machine-learning
tags:
    - performance-measure
---
In a recent post, I have discussed [performance measures for model selection](/post/machine-learning/performance-measures-model-selection/). This time, I write about a related topic: performance measures that are suitable for selecting models when performing feature selection. Since feature selection is concerned with reducing the number of dependent variables, suitable performance measures evaluate the trade-off between the number of features, $p$, and the fit of the model. 

## Performance measures for regression

The mean squared error (MSE) and $R^2$ are unsuited for comparing models during feature selection. This is because, according to these measures, models with more features will always outperform models with fewer features. Instead, the adjusted $R^2$ or Mallow's Cp statistic should be used.

### Adjusted R squared

Given estimates of the outcome $\hat{Y}$ and observed outcomes $Y$, the coefficient of determination can be defined as the square of Pearson's correlation coefficient $\rho$:

\[R^2  = \rho_{\hat{Y}, Y} = \frac{\text{Cov}(\hat{Y}, Y)}{\sigma_{\hat{Y}} \sigma_Y}\]

For models with an intercept, $R^2$ is in the range $[0,1]$. The idea of the adjusted R squared is to adjust $R^2$ according to the number of features $p$. 

!A model with n -p - 1 is the degrees of freedom!

\[R^2_{\rm{adj}} = 1 - \frac{(1 - R^2) (n-1)}{n - p -1} \]

The intuition behind $R^2$ is the following:

* $R^2_{\rm{adj}}$ increases if the enumerator decreases, that is, if $R^2$ is large 
* $R^2_{\rm{adj}}$ increases if the denominator increases, that is, if $p$ is small

This means that $R^2_{\rm{adj}$ only increases when predictors that meaningfully increase $R^2$ are included in the model.

#### Mallow's Cp

Mallow's Cp statistic can be used to assess the fit of least-squares models during feature selection. For Gaussian models, it is identical to the Akaike Information Criterion. Small values of Cp are assigned to models with a good fit. 

The Cp statistic assigns a value of $p+1$ for an ideal model, where $p$ is the number of independent variables. If  $C_p > p+1$, this means that the model is overspecified (i.e. contains too many variables). Assume that there are $k$ available features and you are evaluating a model with $p$ features, then the Cp statistic is defined as:

\[C_p = \frac{SS_{\rm {res}}}{MSE_k} - N + 2 p \]

where $SS_{\rm {res}}$ is the residual sum of squares from the model with $p$ features and $MSE_k$ is the mean-squared error of the model using all of the $k$ features.

## Performance measures for classification

AIC TODO
