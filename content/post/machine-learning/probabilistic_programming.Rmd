---
title: "Probabilistic Programming in R"
author: Matthias DÃ¶ring
date: '2018-11-26'
description: "TODO"
categories:
  - machine-learning
draft: true
---
Probabilistic programming makes it easy to implement statistical models. It is particularly useful for Bayesian models. In this article, I investigate how [Stan](http://mc-stan.org/) can be used through its implementation in R, [RStan](http://mc-stan.org/rstan/).

## Introduction to Stan

Stan is a C++ library for Bayesian inference. It is based on the No-U-Turn sampler (NUTS), which is used for estimating the posterior distribution according to a user-specified model and data. Performing an analysis using Stan involves the following steps

1. Specify the statistical model using the the Stan modeling language. This is typically done through an independent *.stan* file.
2. Prepare the data that is fed to the model.
3. Sample from the posterior distribution. The ```stan``` function automatically compiles the specified model and samples from the specified distributions.
4. Analyze the results.

We will showcase the use of Stan for a hierarchical Bayesian analysis for the eight schools example (Rubin, 1981 and Gelman, 2003). This data set measures the effect of coaching programs on college admission tests, the scholastic aptitude test (SAT), which is used in the US. The SAT was designed in such a way that it should be resistant to short-term efforts directly targeted at improving the score in the test. Rather, the test should reflect the knowledge that was acquired over a
longer period of time. However, for most of the eight schools, the short-term coaching indeed increased the SAT scores as evidenced by positive values of $y_j$, where $y_j$ indicates the change in SAT scores. The data set looks as follows:

|School 	| Estimated effect of coaching ($y_j$) | Standard error of effect ($\sigma_j$) |
|-----------|-----------------|-------------------------|
A| 	28| 	15|
B| 	8 |	10|
C| 	-3| 	16|
D| 	7 |	11|
E| 	-1| 	9|
F| 	1 |	11|
G| 	18| 	10|
H| 	12| 	18|

We are interested in estimating the true effect size for each of the school. There are two alternative approaches that could be used. First, we could assume that all schools are independent of each other. However, this would lead to estimates that are hard to interpret because the 95% posterior intervals for the schools would largely overlap due to the high standard error. Second, one could pool the data from all schools, assuming that the true effect is the same in all schools. This,
however, is also not reasonable because there are different teachers and students at each of the schools. 

Thus, another model is required. The hierarchical model has the advantage of combining information from all eight experiments without assuming that all the $\theta_j$ are equal.
This example is interesting because this is a nontrivial Markov chain simulation problem because there is dependence between the effects of coaching and the variation of the effect in the population. 

We will use the following model:


\[
\begin{align}
y_i &\sim \text{Normal}(\theta_j, \sigma_j)\,, j = 1, \ldots, 8 & \text{Prior for the data}\\
\theta_j &\sim \text{Normal}(\mu, \tau)\,, j = 1, \ldots, 8 & \text{The true effect of the intervention} \\
p(\mu, \tau) &\propto 1 & \text{Parameter distribution is uniform} 
\end{align}
\]

According to the model, the effects of coaching follow a normal distribution whose mean is the true effect, $\theta_j$, and whose standard deviation is the observed deviation of $y_j$, $\sigma_j$, which is known from the data. The true effect, $\theta_j$, follows a normal distribution with parameters $\mu$ and $\tau$.

## Defining the Stan model file

Before defining the Stan program for the model specified above, let us take a look at some statements in Stan. 

### Variables

In Stan, the upper and lower bound of each variable can be denoted in the following way:

```
int<lower=0> n; # lower bound is 0
int<upper=5> n; # upper bound is 5
int<lower=0,upper=5> n; # n is in [0,5]
```

Multi-dimensional data can be specified via square brackets:

```
vector[n] numbers; // a vector of length n
real[n] numbers;  // an array of floats with length n
matrix[n,n] matrix; // an n times n matrix
```

### Program blocks


The following program blocks are used in Stan:

* *data*: for specifying the data that is conditioned upon using Bayes rule
* *transformed data*: for preprocessing the data
* *parameters* (required): for specifying the parameters of the model
* *transformed parameters*: for parameter processing before computing the posterior
* *model* (required): for specifying the model itself
* *generated quantities*: for postprocessing the results


#### The model program block

For the model program block, distributions can be specified in two equivalent ways. The first one, uses the statistical notation:

```
y ~ normal(mu, sigma); # y follows a normal distribution 
```

The second way uses a programmatic notation based on the log probability density function (lpdf):

```
target += normal_lpdf(y | mu, sigma); # increment the normal log density
```

When specifying models via Stan, the ```lookup``` function comes in handy: It provides a mapping from R functions to Stan functions. Consider the following example:

```{r, message = FALSE}
library(rstan) # load stan package
lookup(rnorm)
```

### The eight schools model

With that knowledge, we can define our model, which we will store in a file called ```schools.stan```:

```
data {
  int<lower=0> n; //number of schools
  real y[n]; // effect of coaching
  real<lower=0> sigma[n]; // standard errors of effects
}
parameters {
  real mu;  // the overall mean effect
  real<lower=0> tau; // the inverse variance of the effect
  vector[n] eta; // standardized school-level effects (see below)
}
transformed parameters {
  vector[n] theta; 
  theta = mu + tau * eta; // find theta from mu, tau, and eta
}
model {
  target += normal_lpdf(eta | 0, 1); // eta follows standard normal
  target += normal_lpdf(y | theta, sigma);  // y follows normal with mean theta and sd sigma
}
```

Note that $\theta$ never appears in the parameters. This is because we do not explicitly model $\theta$ but instead model $\eta$, the standardized effect for individual schools. For example, we would expect $\eta_1$ to be large because $y_1$ is the maximum among all schools. We then construct $\theta$ in the *transformed parameters* section according to $\mu$, $\tau$, and $\eta$. This parameterization makes the sampler more efficient.

Note that the model is specified using vector notation since both $\theta$ and $\sigma$ indicate vectors. This allows for improved runtimes because it is not necessary to loop over every individual element of the vectors.

## Preparing the data for modeling

Before we can fit the model, we need to encode the input data as a list whose parameters should correspond to the entries in the data section of the Stan model. For the schools data, the data are the following:

```{r}
schools.data <- list(
  n = 8,
  y = c(28,  8, -3,  7, -1,  1, 18, 12),
  sigma = c(15, 10, 16, 11,  9, 11, 10, 18)
)
```

## Sampling from the posterior distribution

We can sample from the posterior distribution using the ```stan``` function, which performs the following three steps:

1. It translate the model specificiation to C++ code
2. It compiles the C++ code to a shared object
3. It samples from the posterior distribution according to the specified model, data, and settings

If ```rstan_options(auto_write = TRUE)``` has been executed before compiling the model, subsequent calls of the same model will be much faster than the first call because the ```stan``` function then skips the first two steps (translating and compliling the model). Before calling the ```stan``` function, we specify the number of cores we would like to use and allow Stan to store compiled models:

```{r}
options(mc.cores = parallel::detectCores()) # parallelize
rstan_options(auto_write = TRUE)  # store compiled stan model
```

Now, we can compile the model and sample from the posterior. The only two required parameters of ```stan``` are the location of the model file and the data to be fed to the model:

```{r}
fit1 <- stan(
  file = "schools.stan",  # Stan program
  data = schools.data,    # named list of data
  chains = 4,             # number of Markov chains
  warmup = 1000,          # number of warmup iterations per chain
  iter = 2000,            # total number of iterations per chain
  refresh = 1000          # show progress every 'refresh' iterations
  )
```



## Model interpretation

### Basic model interpretation

To retrieve the estimated parameters, we can simply use the ```print``` function.
```{r}
print(fit1) # optional parameters: pars, probs
```

Here, the row names indicate the estimated parameters: mu is the mean of the posterior distribution and tau is its standard deviation. The entries for eta and theta indicate the estimates for the vectors $\eta$ and $\theta$, repectively. The *lp$ entry shows the log density. The columns indicate the computed values. The percentages indicate the credible intervals. For example, the 95% credible interval for $\mu$ is $[-1.27, 18.26]$. Since we are not very certain of the mean, the 95% credible intervals for $\theta_j$ are also quite wide. For example, for the first school, the 95% credible interval is $[-2.19, 32.33]$. 

We can visualize the uncertainty in the estimates using the ```plot``` function:


```{r}
# specify the params to plot via pars
plot(fit1, pars = "theta")
```

The black lines indicate the 95% intervals, while the red lines indicate the 80% intervals. The circles indicate the estimate of the mean. 

### MCMC diagnostics

There are also functions for diagnosing the MCMC procedure. By plotting the trace of the sampling procedure, we can identify whether anything has gone wrong during sampling. This could for example be the case if the chain stays in one place for too long or makes too many steps in one direction. We can plot the traces of the four chains used in our model with the  ```traceplot``` function:

```{r}
# diagnostics:
traceplot(fit1, pars = c("mu", "tau"), inc_warmup = TRUE, nrow = 2)
```

Both traces look fine to me.

We can also obtain the generated samples using the ```extract``` function:

```{r}
# retrieve the samples
samples <- extract(fit1, permuted = TRUE)
mu <- samples$mu  # samples of mu only
# retrieve matrix of iterations, chains, and parameters
chain.data <- extract(fit1, permuted = FALSE) 
print(chain.data[1,,]) # parameters of all chains for the 1st of 1000 iterations
```

To do more advanced analytics of the sampling process, we can use the ```shinystan``` package, which provides a Shiny frontend. A fitted model can be analyzed in the following way:

```{r, eval = FALSE, message = FALSE}
library(shinystan)
launch_shinystan(fit1)
```

## Hierarchical regression

Now that we have a basic understanding of Stan, let's try our hands at hierarchical regression. In conventional regression, we model a relationship of the form

\[Y = \beta_0 + X \beta\]

This representation assumes that all samples have the same distribution. This approach is ill-suited if there is a grouping of the samples because latent differences within and between groups are ignored. An alternative to a single regression model would be to have one regression model for each group. In this case, however, the small sample size would be problematic when estimating individual models. Hierarchical regression is a compromise between both extremes because it assumes that
the groups are similar but still takes differences into account.

Assume that there are $K$ groups. Then, a hierarchical regression is specified as follows:

\[Y_k = \beta_{0k} + X_k \beta^{(k)}\,, \forall k \in {1, \ldots, K\} \]

where $Y_k$ is the outcome for the $k$-th group, $\beta_{0k}$ is the intercept, $X_k$ are the features, and $\beta^{(k)}$ indicates the weights. The hierarchical model is different from a model where $Y_k$ is fit for each group individually because the parameters, $\beta_{0k}$ and $\beta^{(k)}$ are assumed to originate from a common distribution. 

A classic example for hiearchical regression is the rats data set. This longitudinal data set contains the weights of rats as mesaured for 5 weeks. Let us load the data:

```{r}
library(RCurl)
# load data as character
f <- getURL('https://www.datascienceblog.net/data-sets/rats.txt')
# read table from text connection
df <- read.csv(textConnection(f), header=T)
```


## References

https://andrewgelman.com/2014/01/21/everything-need-know-bayesian-statistics-learned-eight-schools/
https://github.com/stan-dev/rstan/wiki/RStan-Getting-Started
http://mc-stan.org/rstan/articles/rstan.html
https://jeremykun.com/2015/04/06/markov-chain-monte-carlo-without-all-the-bullshit/
