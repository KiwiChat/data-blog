---
title: "Linear and Quadratic Discriminant Analysis"
author: Matthias Döring
date: '2018-11-29'
description: ""
draft: true
categories:
  - machine-learning
tags:
    - R
---



<p>The main reason for the popularity of linear discriminant analysis (LDA) is that it allows for transforming data into a (possibly) lower-dimensional space. Given <span class="math inline">\(K\)</span> classes, LDA maps the data to a subspace of <span class="math inline">\(K - 1\)</span> dimensions that is spanned by <span class="math inline">\(K\)</span> centroids.</p>
<div id="the-approach-of-lda" class="section level2">
<h2>The approach of LDA</h2>
<p>LDA can be explained through two perspectives. The first is the probabilistic, Gaussian interpretation and the second is the interpretation as a transformation that is due to Fisher. The first interpretation is useful to understand the underlying assumptions of LDA. The second interpretation gives a better inutition on how the inherent dimensionality reduction of LDA works.</p>
<div id="probabilistic-interpretation" class="section level3">
<h3>Probabilistic interpretation</h3>
<p>Each class <span class="math inline">\(k \in \{1, \ldots, K\}\)</span> is assigned a prior $_k such that that <span class="math inline">\(\sum_{i=1}^k \hat{\pi}_k = 1\)</span>. The posterior probability is</p>
<p><span class="math display">\[\rm{Pr}(G = k |X = x) = \frac{f_k(x) \pi_k}{\sum_{l=1}^K f_l(x) \pi_l} \]</span></p>
<p>where <span class="math inline">\(f_k(x)\)</span> is the density of <span class="math inline">\(X\)</span> conditioned on <span class="math inline">\(k\)</span>. The maximum-a-posterior estimator is</p>
<p><span class="math display">\[G(x) = \arg \max_k \rm{Pr}(G = k | X = x) = \arg \max_k f_k(x) \pi_k \]</span></p>
<p>because the denominator is identical for all classes.</p>
<p>LDA assumes that the density is Gaussian:</p>
<p><span class="math display">\[f_k(x) = \frac{1}{(2 \pi)^{\frac{p}{2}} \sqrt{|\Sigma_k|}} \exp\left(-\frac{1}{2}(x - \mu_k)^T\Sigma_k^{-1}(x - \mu_k)\right)\]</span></p>
<p>where <span class="math inline">\(\Sigma_k\)</span> is the covariance matrix for the samples from class <span class="math inline">\(k\)</span> and <span class="math inline">\(|\Sigma_k|\)</span> is its determinant. LDA assumes that all classes have the same covariance matrices, i.e. <span class="math inline">\(\Sigma_k = \Sigma\,, \forall k\)</span>.</p>
<p>Plugging <span class="math inline">\(f_k\)</span> into the classification function, we find that</p>
<p><span class="math display">\[G(x) = \arg \max_k \delta_k(x)\]</span></p>
<p>where</p>
<p><span class="math display">\[\delta_k(x) = x^T \Sigma^{-1} \hat{\mu}_k -\frac{1}{2} \hat{\mu}_k^T \Sigma^{-1} \mu_k + \log \pi_k \]</span></p>
<p>is the discriminant function for class <span class="math inline">\(k\)</span>. So, now that we have a classifier, how we can compute it?</p>
<p>To find the covariance matrix, we simply compute</p>
<p><span class="math display">\[\Sigma = \sum_{k=1}^K \frac{1}{N - K} \sum_{g_i = k} (x_i - \hat{\mu}_k) (x_i - \hat{\mu}_k)^T\,.\]</span></p>
<p>Note that the deviation from the means is divided by <span class="math inline">\(N-K\)</span>, the degrees of freedom, to obtain an unbiased estimator.</p>
<p>The means of the classes, which are also called centrois, are defined by</p>
<p><span class="math display">\[\hat{\mu}_k = \frac{1}{N_k} \sum_{g_i = k} x_i\,.\]</span></p>
<p>The priors are set to the ratio of the class-specific observations:</p>
<p><span class="math display">\[\hat{\pi}_k = \frac{N_k}{N}\,.\]</span></p>
<p>With this, we have defined all parameters required for the classifier. However, we have not really understood how LDA works: we only know that it is maximizing the posterior according to the Gaussian density. Fortunately, there is an alternative formulation of LDA due to Fisher, which we will briefly deal with next.</p>
</div>
<div id="interpretation-as-a-transformation" class="section level3">
<h3>Interpretation as a transformation</h3>
<p>Fisher’s LDA optimization criterion states that the centroids of the groups should be spread out as far as possible. This amounts to finding a linear combination <span class="math inline">\(Z = a^T X\)</span> such that <span class="math inline">\(a^T\)</span> maximizes the between-class variance relative to the within-class variance.</p>
<p>The within-class variance is <span class="math inline">\(W\)</span> is simply the overall covariance matrix, <span class="math inline">\(\hat{\Sigma}\)</span>. This matrix determines the deviation of all observations from their class centroids. The between-class variance is defined according to the deviation of the centroids from the overall mean, <span class="math inline">\(\hat{\mu} = \sum_{k=1}^K \hat{\pi}_k \hat{\mu}_k\)</span>:</p>
<p><span class="math display">\[B = \sum_{k=1}^K \hat{\pi}_k(\hat{\mu}_k - \hat{\mu}) (\hat{\mu}_k - \hat{\mu})^T\,.\]</span></p>
<p>For <span class="math inline">\(Z\)</span>, the between class variance is <span class="math inline">\(a^T B a\)</span> and the within-class variance is <span class="math inline">\(A^T W a\)</span>. Thus, LDA can be optimized through the Rayleigh quotient</p>
<p><span class="math display">\[\max_a \frac{a^T B a}{a^T W a}\,, \]</span></p>
<p>which defines an optimal mapping of <span class="math inline">\(X\)</span> to the new space <span class="math inline">\(Z\)</span>. Note that $Z ^{1 p}, that is, the observations are mapped to a single dimension. To obtain additional dimensions, we need to solve the optimization problem for <span class="math inline">\(a_1, \ldots, a_{K-1}\)</span> where each successive <span class="math inline">\(a_k\)</span> is constructed to be orthogonal in <span class="math inline">\(W\)</span> to the previous discriminant coordinates. This leads to the linear transformation <span class="math inline">\(G = (Z_1^T, Z_2^T, \ldots, Z_{K-1}^T) \in \mathbb{R}^{p \times q}\)</span> with which we can map <span class="math inline">\(X\)</span> from <span class="math inline">\(p\)</span> to <span class="math inline">\(q\)</span> dimension via <span class="math inline">\(X G\)</span>. Why do we consider <span class="math inline">\(K-1\)</span> projections? This is because the affine subspace that is spanned by the <span class="math inline">\(K\)</span> centroids has a rank of at most <span class="math inline">\(K-1\)</span> due to the spherical assumptions of LDA.</p>
<p>The benefit of this formulation of LDA is that is enables us to perform classification in a reduced subspace. We do not need to use the full <span class="math inline">\(K-1\)</span> dimensions but can choose a smaller subspace <span class="math inline">\(l &lt; K-1\)</span>. This is called reduced-rank LDA.</p>
<p>Using this formulation of LDA, classification involves two steps:</p>
<ol style="list-style-type: decimal">
<li>Sphere the data using the common covariance matrix <span class="math inline">\(\hat{\Sigma} = UDU^T\)</span> (eigendecomposition) such that <span class="math inline">\(X^{\ast} = D^{-\frac{1}{2}} U^T X\)</span>. In this way, the covariance of <span class="math inline">\(X^{\ast}\)</span> becomes the identity matrix. By eliminating the covariance between the variables through sphering the data, it becomes much easier for separation of classes in the transformed space.</li>
<li>Classify observations <span class="math inline">\(x_i\)</span> to the closest class centroid in the transformed space, taking into account the class priors <span class="math inline">\(\pi_k\)</span>. The intuition about the <span class="math inline">\(\pi_k\)</span> is that if an observation would have equal distance to the centroids from two classes, then it would be assigned to the class with the greater prior.</li>
</ol>
<p><a href="https://machinelearningmastery.com/linear-discriminant-analysis-for-machine-learning/" class="uri">https://machinelearningmastery.com/linear-discriminant-analysis-for-machine-learning/</a></p>
</div>
</div>
<div id="complexity-of-the-lda-model" class="section level2">
<h2>Complexity of the LDA model</h2>
<p>The number of effective parameters of LDA can be derived in the following way.
There are <span class="math inline">\(K\)</span> means, <span class="math inline">\(\hat{\mu}_k\)</span> that are estimated. Since the covariance matrix is specified by the centroids alone, there are no additional parameters for <span class="math inline">\(\Sigma\)</span>. Since we only need to consider the differences <span class="math inline">\(\delta_k(x) - \delta_K(x)\)</span> of discriminant function, this gives rise to <span class="math inline">\(K-1\)</span> calculations involving the <span class="math inline">\(p\)</span> elements of the centroids and the prior. Thus, the total number of parameters is <span class="math inline">\(K-1 \cdot (p+1)\)</span>.</p>
</div>
<div id="the-phoneme-data-set" class="section level2">
<h2>The phoneme data set</h2>
<p>We will use the <a href="https://web.stanford.edu/~hastie/ElemStatLearn/datasets/phoneme.info.txt">phoneme speech recognition data set</a>:</p>
<pre class="r"><code>library(RCurl)
f &lt;- getURL(&#39;https://www.datascienceblog.net/data-sets/phoneme.csv&#39;)
df &lt;- read.csv(textConnection(f), header=T)
print(dim(df))</code></pre>
<pre><code>## [1] 4509  259</code></pre>
<p>The data set contains samples of digitized speech for five phonemes: aa (as the vowel in <em>dark</em>), <em>ao</em> (as the first vowel in <em>water</em>), <em>dcl</em> (as in <em>dark</em>), <em>iy</em> (as the vowel in <em>she</em>), and <em>sh</em> (as in <em>she</em>). In total, 4509 speech frames of 32 msec were selected. For each speech frame, a log-periodogram of length 256 was computed, which is suitable for speech recognition. The 256 columns labeled <em>x.1</em> to <em>x.256</em> identify the speech features, while the columns <em>g</em> and <em>speaker</em> indicate the phonemes (labels) and speakers, respectively.</p>
<p>Let us assign training and test samples:</p>
<pre class="r"><code>#logical vector: TRUE if entry belongs to train set, FALSE else
train &lt;- grepl(&quot;^train&quot;, df$speaker)
# remove non-feature columns
to.exclude &lt;- c(&quot;row.names&quot;, &quot;speaker&quot;, &quot;g&quot;)
feature.df &lt;- df[, !colnames(df) %in% to.exclude]
test.set &lt;- subset(feature.df, !train)
train.set &lt;- subset(feature.df, train)
train.responses &lt;- subset(df, train)$g
test.responses &lt;- subset(df, !train)$g</code></pre>
</div>
<div id="fitting-an-lda-model-in-r" class="section level2">
<h2>Fitting an LDA model in R</h2>
<p>We can fit an LDA model in the following way:</p>
<pre class="r"><code>library(MASS)
lda.model &lt;- lda(train.set, grouping = train.responses)</code></pre>
<p>We can transform the training data to the canonical coordinates in the following way:</p>
<pre class="r"><code>lda.prediction.train &lt;- predict(lda.model, train.set)
x.projected &lt;- lda.prediction.train$x # feature transformation 
# visualize the features in the two LDA dimensions
plot.df &lt;- data.frame(x.projected[,1:2], &quot;Outcome&quot; = train.responses)
library(ggplot2)
ggplot(plot.df, aes(x = LD1, y = LD2, color = Outcome)) + geom_point()</code></pre>
<p><img src="/post/machine-learning/linear-discriminant-analysis_files/figure-html/unnamed-chunk-4-1.png" width="672" /></p>
<p>Plotting the data in the two LDA dimensions reveals three clusters:</p>
<ul>
<li>Cluster 1 (left) consists of <em>aa</em> and <em>ao</em> phonemes</li>
<li>Cluster 2 (bottom right) consists of <em>dcl</em> and <em>iy</em> phonemes</li>
<li>Cluster 3 (top right) consists of <em>sh</em> phonemes</li>
</ul>
<p>This indicates that two dimensions are not sufficient for differentiating all 5 classes. However, the clustering indicates that phonemes that are sufficiently different from one another can be differentiated very well.</p>
<p>We can also plot the mapping of training data onto the discriminant dimensions using the <code>plot.lda</code> function where the <em>dimen</em> parameter can be used to specify the number of considered dimensions:</p>
<pre class="r"><code>colors &lt;- c(&quot;red&quot;, &quot;blue&quot;, &quot;green&quot;, &quot;yellow&quot;, &quot;black&quot;) # aa, ao, dcl, iy, sh
my.cols &lt;- colors[match(lda.prediction.train$class, levels(df$g))]
plot(lda.model, dimen = 4, col = my.cols)</code></pre>
<p><img src="/post/machine-learning/linear-discriminant-analysis_files/figure-html/unnamed-chunk-5-1.png" width="672" /></p>
<p>Plotting the training data for all dimension pairs demonstrates that, by construction, the first two dimensions separate the phoneme groups best. What we can learn from the plot, though, is how many dimensions we should select for a reduced-rank LDA. Remember that LD1 and LD2 confused <em>aa</em> with <em>ao</em> and <em>dcl</em> with <em>iy</em>. Thus, we would like additional dimensions that help us differentiating these groups. It seems that we need all of the four dimensions because <em>dcl</em> and <em>iy</em> are only well-separed in LD1 vs LD3, while <em>aa</em> and <em>ao</em> are only well-separated when LD4 is combined with any of the other dimensions.</p>
<p>To visualize the centroids of the groups, we can create a custom plot:</p>
<pre class="r"><code>plot_lda_centroids &lt;- function(lda.model, train.set, response) {
    centroids &lt;- lda.model$means %*% lda.model$scaling 
    library(RColorBrewer)
    colors &lt;- brewer.pal(8, &quot;Accent&quot;)
    my.cols &lt;- colors[match(lda.prediction.train$class, levels(df$g))]
    my.points &lt;- as.matrix(train.set) %*% lda.model$scaling
    no.classes &lt;- length(levels(response))
    par(mfrow=c(no.classes -1, no.classes -1), mar=c(1,1,1,1), oma=c(1,1,1,10))
    for (i in 1:(no.classes - 1)) {
        for (j in 1:(no.classes - 1)) {
            y &lt;- my.points[, i]
            x &lt;- my.points[, j]
            cen &lt;- cbind(centroids[, j], centroids[, i])
            if (i == j) {
                plot(x, y, type=&quot;n&quot;) 
                max.y &lt;- max(my.points[, i])
                max.x &lt;- max(my.points[, j])
                min.y &lt;- min(my.points[, i])
                min.x &lt;- min(my.points[, j])
                max.both &lt;- max(c(max.x, max.y))
                min.both &lt;- max(c(min.x, min.y))
                center &lt;- min.both + ((max.both - min.both) / 2)
                text(center, center, colnames(my.points)[i], cex = 3)}
            else {
                plot(x, y, col = my.cols, pch = as.character(response), xlab =&quot;&quot;, ylab=&quot;&quot;)
                points(cen[,1], cen[,2], pch = 21, col = &quot;black&quot;, bg = colors, cex = 3)
            }
        }
    }
    par(xpd = NA)
    legend(x=par(&quot;usr&quot;)[2] + 1, y = mean(par(&quot;usr&quot;)[3:4]) + 20, 
            legend = rownames(centroids), col = colors, pch = rep(20, length(colors)), cex = 3)
}
plot_lda_centroids(lda.model, train.set, train.responses)</code></pre>
<p><img src="/post/machine-learning/linear-discriminant-analysis_files/figure-html/unnamed-chunk-6-1.png" width="1152" /></p>
</div>
<div id="reduced-rank-lda" class="section level2">
<h2>Reduced-rank LDA</h2>
<p>Although this data set is not optimal for reduced-rank LDA since it seems that all dimensions are worthwhile, we will still showcase the use of reduced-rank LDA by evaluating the accuracy of LDA based on one to four discriminant coordinates:</p>
<pre class="r"><code># perform LDA with 1 to 4 discriminant coordinates
dims &lt;- 1:4
accuracies &lt;- rep(NA, length(dims))
for (i in seq_along(dims)) {
    lda.pred &lt;- predict(lda.model, test.set, dim = dims[i])
    acc &lt;- length(which(lda.pred$class == test.responses))/length(test.responses)
    accuracies[i] &lt;- acc
}
print(accuracies)</code></pre>
<pre><code>## [1] 0.5141146 0.7100086 0.8622754 0.9195894</code></pre>
<p>As expected from the visual exploration of the transformed space, the test accuracy increases with each additional dimension. Since LDA with four dimension obtains the maximal accuracy, we would decide to use all of the discriminant coordinates for classification.</p>
<p>We can visualize the performance of the classifier using four dimensions:</p>
<pre class="r"><code>lda.pred &lt;- predict(lda.model, test.set)
plot.df &lt;- data.frame(lda.pred$x[,1:2], &quot;Outcome&quot; = test.responses, 
                    &quot;Prediction&quot; = lda.pred$class)
ggplot(plot.df, aes(x = LD1, y = LD2, color = Outcome, shape = Prediction)) +
        geom_point()</code></pre>
<p><img src="/post/machine-learning/linear-discriminant-analysis_files/figure-html/unnamed-chunk-8-1.png" width="672" /></p>
<p>In the plot, the labels are indicated by different colors and the predictions by different colors. Ideally, each color would have only a single symbol. Incorrect predictions are visible when one colors exhibits different symbols. Using the plot, we quickly see that most confusions occur when observations labeled as <em>aa</em> are incorrectly classified as <em>ao</em> and vice versa.</p>
</div>
<div id="qda" class="section level2">
<h2>QDA</h2>
<p>QDA has (K-1)[P(P+3)/2+1] parameters</p>
</div>
