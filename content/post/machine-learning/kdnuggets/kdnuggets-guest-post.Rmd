---
title: "Supervised Learning: Model Popularity from Past to Present"
author: Matthias DÃ¶ring
date: '2018-11-26'
description: "The field of machine learning has changed enormously in the last decades. Here, I investigate the popularity of supervised learning techniques through time and across different scientific fields."
draft: true
categories:
  - machine-learning
output:
    html_document:
        self_contained: no
---
The field of machine learning has gone through enormous changes in the last decades. Admittedly, there are some methods that have been around for a long time and are still a staple of the field. For example, the concept of least squares was already proposed in the early 19th century by [Legendre](https://archive.org/details/nouvellesmthode00legegoog/) and [Gauss](https://www.e-rara.ch/zut/content/titleinfo/142787). Other approaches such as neural networks, [whose most basic form was introduced in 1958](http://psycnet.apa.org/record/1959-09865-001), were substantially advanced in the last decades, while other methods such as [support vector machines (SVMs)](https://link.springer.com/article/10.1007/BF00994018) are even more recent.

Due to the large number of available approaches for supervised learning, the following question is often posed: **What is the best model?** As we all know, this question is hard to answer because, as George Box famously stated, *[a]ll models are wrong but some are useful*. In particular, the usefulness of the model critically depends on the data at hand. Thus, there is no general answer to this question. A question that is easier to answer is the following: **What is the most popular model?**. This will be the concern of this article. 

## Measuring the popularity of machine learning models

For the purposes of this article, I will define popularity using a frequentist approach. More precisely, I will use the number of scientific publications mentioning individual supervised learning models as a surrogate for popularity. Of course, there are some limitations to this approach:

* There are probably more accurate notions of popularity than the number of publications. For example, publications criticizing a certain model do not necessarily imply that the model is popular.
* The analysis is influenced by the search terms that are used. To ensure high specificities, I did not consider model abbreviations, which is why I likely did not retrieve all potential hits. Moreover, sensitivity may be low for models that are also referenced by search terms that were not considered in the analysis.
* Literature databases are not perfect: sometimes, publications are stored with incorrect metadata (e.g. incorrect years) or there may be duplicate publications. Thus, some noise in the publication frequencies is to be expected.

For this piece, I performed two analyses. The first analysis is a longitudinal analysis of publication frequencies, while the second analysis compares the overall number of publication associated with machine learning models across different fields. 

For the first analysis, I determined the number of publications by scraping data from Google Scholar, which considers the titles and abstracts of scientific publications. To identify the number of publications associated with individual supervised learning approaches, I determined the number of Google Scholar hits between 1950 and 2017. Since data scraping from Google Scholar is notoriously hard, I relied on [helpful advice from ScrapeHero](https://www.scrapehero.com/how-to-fake-and-rotate-user-agents-using-python-3/) to gather the data.

I included the following 13 supervised approaches in the analysis: neural networks, deep learning, SVMs, random forests, decision trees, linear regression, logistic regression, Poisson regression, ridge regression, lasso regression, k-nearest neighbors, linear discriminant analysis, and log-linear models. Note that for lasso regression, the terms *lasso regression* and *lasso model* were considered. For nearest neighbors, the terms *k-nearest neighbor* and *k-nearest neighbour* were considered. The resulting data set indicates the [number of publications associated with each supervised model from 1950 until now](https://www.datascienceblog.net/data-sets/ml_models_timeline.csv).

## Use of supervised models from 1950 until now

To analyze the longitudinal data, I will differentiate two periods: the early days of machine learning (1950 to 1980), in which few models were available, and the formative years (1980 until now), in which interest in machine learning surged and many new models were developed. Note that in the following visualizations only the most relevant methods are shown.

```{r, echo = FALSE, fig.height = 8}
get.top.models <- function(df, k) {
    library(plyr) 
    total.df <- ddply(df, "Model", summarize, Total = sum(Count))
    o <- order(total.df$Total, decreasing = TRUE)
    top.models <- total.df[o, "Model"][1:k]
    return(top.models)
}
library(plyr)
df <- read.csv("ml_models_timeline.csv")
total.pubs <- ddply(df, .(Year), summarize, Total = sum(Count))
most.pubs.in.year <- total.pubs$Year[which.max(total.pubs$Total)]

# plot all and for all time
library(ggplot2)
#ggplot(df, aes(x = Year, y = Count, color = Model)) + 
#        geom_point() + geom_line() + ggtitle("Overall")
# select only the top-k models overall (from 1950 to now)
k <- 6
top.models <- get.top.models(df, k) # overall top models
plot.df <- df[df$Model %in% top.models, ]
library(ggplot2)
early.idx <- which(df$Year >= 1950 & df$Year < 1980)
top.models.early <- get.top.models(df[early.idx, ], k) # early top models
### Adjust df to pool DL + Neural Network
add.df <- df[df$Model == "Neural Network", ]
add.df$Count <- add.df$Count + df$Count[df$Model == "Deep Learning"]
add.df$Model <- "Neural Network/Deep Learning"
df <- rbind(df, add.df)
###
p.df.early <- df[early.idx, ]
early.summary <- ddply(p.df.early, "Model", summarize, Total = sum(Count))
o <- order(early.summary$Total, decreasing = TRUE)
#p.df.early$Model <- factor(p.df.early$Model, levels = early.summary$Model[o])
#p.df.early <- p.df.early[p.df.early$Model %in% top.models.early, ]
late.idx <- which(df$Year >= 1980 & df$Year <= 2017)
top.models.late <- get.top.models(df[late.idx, ], k) # late top models
# include deep learning
#top.models.late <- c(as.character(top.models.late), "Deep Learning")
top.models.late <- c("Decision Tree", "Deep Learning", "Linear Regression", "Logistic Regression", "Neural Network", "Neural Network/Deep Learning", "Support Vector Machine")
p.df.late <- df[late.idx, ]
late.summary <- ddply(p.df.late, "Model", summarize, Total = sum(Count))
o <- order(late.summary$Total, decreasing = TRUE)
p.df.late$Model <- factor(p.df.late$Model, levels = late.summary$Model[o])
p.df.late <- p.df.late[p.df.late$Model %in% top.models.late, ]
p.df.early$Model <- factor(p.df.early$Model, levels = late.summary$Model[o])
p.df.early <- p.df.early[p.df.early$Model %in% top.models.late, ]
#p.df.late <- p.df.late[p.df.late$Model %in% top.models, ]
p2 <- ggplot(p.df.late, aes(x = Year, y = Count, color = Model)) + 
        geom_point() + geom_line() + 
        ggtitle("Formative years of machine learning: 1980 until now")
library(RColorBrewer)
pair.colors <- brewer.pal(4, "Paired")
other.colors <- brewer.pal(9, "Set3")
my.colors1 <- c("Neural Network/Deep Learning" = pair.colors[2],
               "Deep Learning" = other.colors[5],
               "Neural Network" = pair.colors[2],
               "Linear Regression" = pair.colors[4],
               "Logistic Regression" = pair.colors[3])
my.colors2 <- c("Support Vector Machine" = other.colors[6], 
                  "Decision Tree" = other.colors[4])
my.colors <- c(my.colors1, my.colors2)

plot_timeline <- function(data, title, caption, my.colors) {
    p <- ggplot(data, aes(x = Year, y = Count, color = Model, shape = Model)) + 
        geom_point() + geom_line() + 
        ggtitle(title) +
        labs(caption = caption)  +
        scale_color_manual(values = my.colors) +
        ylab("Number of publication mentions")
    return(p)
}
p1 <-  plot_timeline(p.df.early, "Early days of machine learning: 1950 to 1980", 
        "Figure 1: Overall publication rates of machine learning models from 1950 to 1980.", my.colors) 
p.df.late <- p.df.late[p.df.late$Model != "Neural Network",] # show only pooled values for DL and NN
p4 <- plot_timeline(p.df.late, "Formative years of machine learning: 1980 until now", 
                    "Figure 2: Overall publication rates for machine learning models from 1980 until now.", my.colors)
#library(gridExtra)
#grid.arrange(p1, p2, p3)
```

### Early days: dominance of linear regression

<img src = "figure1-early-ml.png" width = "700" height = "500" alt = "Early days of ML">

```{r, echo = FALSE}
#p1
```

As we can see from Figure 1, linear regression was the dominating method between 1950 and 1980. In comparison, other machine learning models were mentioned extremely rarely in the scientific literature. Starting from the 1960s, however, we can see that the popularity of neural networks and decision trees began to grow. We can also see that logistic regression was not widely available yet, with only slight increases in the number of mentions at the end of the 1970s. 

<img src = "figure2-formative-ml.png" width = "700" height = "500" alt = "Formative years of ML">

```{r, echo = FALSE}
#p4
```

### Formative years: diversification and rise of neural networks

Figure 2 demonstrates that the supervised models that were mentioned in scientific publiations became considerably more diverse starting from the late 1980s. Importantly, the rate at which machine learning models were mentioned in the scientific literature has steadily increased until 2013. The plot particularly demonstrates the popularity of linear regression, logistic regression, and neural networks. As we have seen before, linear regression has already been popular way before 1980. In 1980, however, the popularity of neural networks and logistic regression started growing rapidly. While the popularity of logistic regression reached its peak in 2010 where the method nearly became as popular as linear regression, the pooled popularity of neural networks and deep learning (curve *Neural Network/Deep Learning* in Figure 2) even surpassed the popularity of linear regression in 2015. 

Neural networks have become immensely popular because they have led to breakthroughs in machine learning applications such as image recognition ([ImageNet](http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks), 2012), face recognition ([DeepFace](https://www.cv-foundation.org/openaccess/content_cvpr_2014/html/Taigman_DeepFace_Closing_the_2014_CVPR_paper.html), 2014), and gaming
([AlphaGo](https://ai.googleblog.com/2016/01/alphago-mastering-ancient-game-of-go.html), 2016). The data from Google Scholar suggest that the frequency at which neural networks have been mentioned in scientific articles has slightly decreased in the last years (not shown in Figure 2). This is likely since the term *deep learning* (multilayered neural networks) has supplanted the use of the term *neural network* to some extent. The same finding can be made using [Google Trends](https://trends.google.com/trends/explore?date=all&q=deep%20learning,neural%20network). 

The remaining, slightly less popular supervised approaches are decision trees and SVMs. In contrast to the top-3 methods, the rates at which these approaches are mentioned are decidedly smaller. On the other hand, there also seems to be less fluctuation in the frequency at which these methods are mentioned in the literature. Notably, neither the popularity of decision trees nor SVMs has yet declined. This is in contrast to other methods such as linear and logistic regression whose number of mentions has decreased considerably in the last years. Between decision trees and SVMs, SVM mentions seem to exhibit the more favorable growth trend as SVMs managed to overtake decision trees merely 15 years after their invention.

The number of mentions of the considered machine learning models peaked in 2013 (589,803 publications) and has slightly decreased since then (462,045 publications in 2017).

## Popularity of supervised learning models across different fields

In the second analysis, I wanted to investigate whether different communities rely on different machine learning techniques. For this purpose, I relied on three repositories for scientific publications: [Google Scholar](https://scholar.google.com) for general publications, [dblp](https://dblp.uni-trier.de/) for computer science publications, and [PubMed](https://www.ncbi.nlm.nih.gov/pubmed/) for publications in the biomedical sciences. In each of the three repositories, I determined [the frequency at which hits for the 13 machine learning models were found](https://www.datascienceblog.net/data-sets/ml_models_overall.csv). The results are shown in Figure 3.

```{r, message = FALSE, echo = FALSE, warning = FALSE}
df <- read.csv("ml_models_overall.csv")
library(reshape2)
df.m <- melt(df, "Source")
# get percentages: values are not comparable otherwise
library(dplyr)
df.m <- group_by(df.m, Source) %>% mutate(percent = value/sum(value, na.rm = TRUE))
colnames(df.m) <- c("Source", "Model", "Count", "Percentage")
# sort columns by overall usage
overall.idx <- which(df.m$Source == "Overall (Google Scholar)")
o <- order(df.m$Count[overall.idx], decreasing = TRUE)
df.m$Model <- factor(gsub("[_.]", " ", df.m$Model), levels = gsub("[_.]", " ", levels(df.m$Model))[o])
p.fields <- ggplot(df.m, aes(x = Model, y = Percentage, fill = Source)) + geom_bar(stat = "identity") +
    theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5)) + 
    scale_y_continuous(labels = scales::percent_format()) +
    ggtitle("Use of machine learning models in different fields") + 
    labs(caption = "Figure 3: Rates at which supervised models are mentioned across different fields.")
#p.fields
# top 6 models by source
#n <- 6
#for (source in unique(df.m$Source)) {
    #idx <- which(df.m$Source == source)
    #o <- order(df.m$Percentage[idx], decreasing = TRUE)
    #top.df <- df.m[idx[o[1:n]], ]
    #print(top.df)
#}
```
<img src = "figure3-ml-fields.png" width = "700" height = "500" alt = "ML models in different fields">

Figure 3 demonstrates that many methods are quite specific to individual fields. In the following, let us analyze the most popular models in each field.

### Overall use of supervised learning models

According to Google Scholar, these are the five most frequently used supervised models:

1. **Linear regression:** 3,580,000 (34.3%) papers
2. **Logistic regression:** with 2,330,000 (22.3%) papers
3. **Neural networks:** 1,750,000 (16.8%) papers
4. **Decision trees:** 875,000 (8.4%) papers
5. **Support vector machines:** with 684,000 (6.6%) papers

Overall, linear models are clearly dominating, making up more than 50% of hits for supervised models. Non-linear methods are not far behind though: neural networks make third place with 16.8% of all papers, followed by decision trees (8.4% of papers) and SVMs (6.6% of papers).

### Use of models in the biomedical sciences

According to PubMed, the five most popular machine learning models in the biomedical sciences are:

1. **Logistic regression:** 229,956 (54.5%) papers
2. **Linear regression:** 84,850 (20.1%) papers
3. **Cox regression:** 38,801 (9.2%) papers
4. **Neural networks:** 23,883 (5.7%) papers
5. **Poisson regression:** 12,978 (3.1%) papers

In the biomedical sciences, we see an overrepresentation in the number of mentions relating to linear models: four of of the five most popular methods are linear. This is probably due to two
reasons. First, in medical settings, the number of samples is often too small to allow for fitting complex non-linear models. Second, the ability to interpret the results is critical for medical applications. Since non-linear methods are typically harder to interpret, they are less suited for medical applications because high predictive performance alone is typically not sufficient.

The popularity of logistic regression in the PubMed data is likely due to the large number of publications presenting clinical studies. In these studies, the categorical outcome (i.e. treatment success) is often analyzed using logistic regression because it is well-suited for interpreting the impact of the features on the outcome. 
Note that that Cox regression is so popular in the PubMed data because it is frequently used for analyzing Kaplan-Meier survival data.

### Use of models in computer science

The five most popular models in the computer science bibliography retrieved from dblp are:

1. **Neural networks:** 63,695 (68.3%) papers
2. **Deep learning:** 10,157 (10.9%) papers
3. **Support vector machines:** 7,750 (8.1%) papers
4. **Decision trees:** 4,074 (4.4%) papers
5. **Nearest neighbors:** 3,839 (2.1%) papers

The distribution of machine learning models mentioned in computer science publications is quite distinct: the majority of publications seems to deal with recent, non-linear methods (e.g. neural networks, deep learning, and support vector machines). If we include deep learning, more than three quarters of the retrieved computer science publications deal with neural networks.

### A cleavage between communities

```{r, echo = FALSE, warning = FALSE}
###
# group parametric and non-parametric methods
###
# param.methods: parametric and semi-parametric (e.g. cox)
param.methods <- c("Linear Regression", "Logistic Regression", "Decision Tree", "Cox Regression", 
                    "Poisson Regression", "Linear Discriminant Analysis", "Log Linear Model", 
                    "Ridge Regression", "Lasso Regression")
non.param.methods <- c("Neural Network", "Support Vector Machine", "Deep Learning",
                        "Nearest Neighbor", "Random Forest")
df.m$Class <- ifelse(df.m$Model %in% param.methods, "Parametric", "Non-Parametric")
p.parametric <- ggplot(df.m, aes(x = Source, y = Percentage, fill = Class)) + geom_bar(stat = "identity") +
    theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5)) + 
    scale_y_continuous(labels = scales::percent_format()) +
    ggtitle("Use of machine learning models in different fields") + 
    labs(caption = "Figure 4: Use of non-parametric and parametric models in different fields.")
#p.parametric
# total percentages
percentage.df <- ddply(df.m, .(Source, Class), summarize, Total = sum(Percentage))
```
<img src = "figure4-ml-fields.png" width = "700" height = "500" alt = "Types of ML models in different fields">

Figure 4 summarizes the percentage of parametric (including semi-parametric) and non-parametric models that are mentioned in the literature. The bar plot demonstrates that there is a big difference between the models investigated in machine learning research (as evidenced by computer science publications) and the types of models that are applied (as evidenced by biomedical and overall publications). While more than 90% of computer science publications deal with non-parametric models, roughly 90% of biomedical publications deal with parametric models. This showcases that machine learning research is heavily focused on state-of-the-art methods such as deep neural networks, while users of machine learning often rely on more interpretable, parametric models.

## Summary

This analysis of mentions of individual supervised learning models in the scientific literature showcases the high popularity of artificial neural networks. However, we also see that different types of machine learning models are used in different fields. Particularly researchers in the biomedical sciences still heavily rely on parametric models. It will be interesting to see whether more complex models will find widespread use in the biomedical field or whether these models are simply not appropriate for typical applications in this sector (e.g. due to insufficient interpretability of these models and low generalizability when the sample size is small).
