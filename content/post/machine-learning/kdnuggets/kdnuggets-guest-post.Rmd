---
title: "Supervised Learning: Model Popularity from Past to Present"
author: Matthias DÃ¶ring
date: '2018-11-26'
description: "The field of machine learning has changed enormously in the last decades. Here, I investigate the popularity of supervised learning techniques through time and across different scientific fields."
draft: true
categories:
  - machine-learning
---
The field of machine learning has gone through enormous changes in the last decades. Admittedly, there are some methods that have been around for a long time and have not changed much. For example, the concept of least squares was already proposed in the early 19th century by [Legendre](https://archive.org/details/nouvellesmthode00legegoog/) and [Gauss](https://www.e-rara.ch/zut/content/titleinfo/142787). Other approaches such as neural networks, [whose most basic form was introduced in 1958](http://psycnet.apa.org/record/1959-09865-001), were substantially advanced in the last decades and there are also more recently developed methods such as [support vector machines (SVMs)](https://link.springer.com/article/10.1007/BF00994018).

Due to the large number of available approaches for supervised learning, the following question is often posed: **What is the best model?** As we all know, this question is hard to answer because *[a]ll models are wrong but some are useful* (George Box). Moreover, the usefulness of the model critically depends on the data at hand. Thus, there is no general answer to this question. A question that is, easier to answer is the following: **What is the most popular model?**

## Measuring the popularity of machine learning models

For the purposes of this article, I will define popularity using a frequentist approach. More precisely, I will use the number of scientific publications associated with individual supervised learning models as a surrogate for popularity. Of course, there are some limitations to this approach:

* There are probably more accurate notions of popularity than the number of publications. For example, publications criticizing a certain model do not necessarily imply that the model is popular.
* The analysis is influenced by the search terms that are used. To ensure high specificities, I do not consider model abbreviations. Moreover, sensitivity may be low for models that are also referenced by search terms that were not included in the analysis.
* Literature databases are not perfect: sometimes, publications are stored with incorrect metadata (e.g. incorrect years) or there may be duplicate publications. Thus, some noise in the publication frequencies is to be expected.

For this piece, I performed two analyses. The first analysis is a longitudinal analysis of the publication frequencies, while the second analysis compares the overall number of publication associated with machine learning models in different fields. 

For the first analysis, I determined the number of publications by scraping data from searches with Google Scholar, which searches the titles and abstracts of scientific publications. To identify the number of publications associated with individual supervised learning approaches, I determined the number of Google Scholar hits between 1950 and 2018. Since Google does not allow automated requests, I found [helpful advice from ScrapeHero](https://www.scrapehero.com/how-to-fake-and-rotate-user-agents-using-python-3/) to circumvent this problem.

I included the following 12 supervised models in the analysis: neural networks, SVMs, random forests, decision trees, linear regression, logistic regression, Poisson regression, ridge regression, lasso regression, k-nearest neighbors, linear discriminant analysis, and log-linear models. Note that for lasso regression, the terms *lasso regression* and *lasso model* were considered. For nearest neighbors, the terms *k-nearest neighbor* and *k-nearest neighbour* were considered. The resulting data set indicates the [number of publications associated with each of the twelve supervised models from 1950 until now](https://www.datascienceblog.net/data-sets/ml_models_timeline.csv).

## Use of supervised models from 1950 until now

To analyze the longitudinal data, I will differentiate two periods: the early days of machine learning (1950 to 1980), in which few models were available, and the formative years (1980 until now), in which interest in machine learning surged and many new models were developed.

```{r, echo = FALSE, fig.height = 8}
get.top.models <- function(df, k) {
    library(plyr) 
    total.df <- ddply(df, "Model", summarize, Total = sum(Count))
    o <- order(total.df$Total, decreasing = TRUE)
    top.models <- total.df[o, "Model"][1:k]
    return(top.models)
}
library(plyr)
df <- read.csv("ml_models_timeline.csv")
total.pubs <- ddply(df, .(Year), summarize, Total = sum(Count))
most.pubs.in.year <- total.pubs$Year[which.max(total.pubs$Total)]

# plot all and for all time
library(ggplot2)
#ggplot(df, aes(x = Year, y = Count, color = Model)) + 
#        geom_point() + geom_line() + ggtitle("Overall")
# select only the top-k models overall (from 1950 to now)
k <- 6
top.models <- get.top.models(df, k) # overall top models
plot.df <- df[df$Model %in% top.models, ]
library(ggplot2)
early.idx <- which(df$Year >= 1950 & df$Year < 1980)
top.models.early <- get.top.models(df[early.idx, ], k) # early top models
p.df.early <- df[early.idx, ]
early.summary <- ddply(p.df.early, "Model", summarize, Total = sum(Count))
o <- order(early.summary$Total, decreasing = TRUE)
p.df.early$Model <- factor(p.df.early$Model, levels = early.summary$Model[o])
p.df.early <- p.df.early[p.df.early$Model %in% top.models.early, ]
#p.df.early <- p.df.early[p.df.early$Model %in% top.models, ]
p1 <- ggplot(p.df.early, aes(x = Year, y = Count, color = Model)) + 
        geom_point() + geom_line() + 
        ggtitle("Early days of machine learning: 1950 to 1980")
# zoom into the plot, starting from the 80s
late.idx <- which(df$Year >= 1980)
top.models.late <- get.top.models(df[late.idx, ], k) # late top models
# include deep learning
top.models.late <- c(as.character(top.models.late), "Deep Learning")
p.df.late <- df[late.idx, ]
late.summary <- ddply(p.df.late, "Model", summarize, Total = sum(Count))
o <- order(late.summary$Total, decreasing = TRUE)
p.df.late$Model <- factor(p.df.late$Model, levels = late.summary$Model[o])

p.df.late <- p.df.late[p.df.late$Model %in% top.models.late, ]
#p.df.late <- p.df.late[p.df.late$Model %in% top.models, ]
p2 <- ggplot(p.df.late, aes(x = Year, y = Count, color = Model)) + 
        geom_point() + geom_line() + 
        ggtitle("Formative years of machine learning: 1980 until now")
# summarize neural networks and deep learning
p.df.sum <- p.df.late
p.df.sum$Count[p.df.late$Model == "Neural Network"] <- p.df.late$Count[p.df.late$Model == "Neural Network"] + p.df.late$Count[p.df.late$Model == "Deep Learning"]
p.df.sum <- p.df.sum[p.df.sum$Model != "Deep Learning",]
p3 <- ggplot(p.df.sum, aes(x = Year, y = Count, color = Model)) + 
        geom_point() + geom_line() + 
        ggtitle("Formative years of machine learning: 1980 until now")
#library(gridExtra)
#grid.arrange(p1, p2, p3)
```

### Dominance of linear regression, logistic regression, and neural networks

```{r, echo = FALSE, fig.cap="\\label{fig:models_early}Overall publication rates of machine learning models from 1950 to 1980."}
p1
```

Linear regression was particularly dominant from 1950 to 1980 (Figure 1). During this time, other methods were rarely used, while the use of linear regression became more and more common (Figure 1). 

```{r, echo = FALSE, fig.cap="\\label{fig:models_late}Overall publication rates of machine learning models from 1980 until now."}
p2
```

The publication frequencies became considerably more diverse starting from the late 1980s (Figure 2) when neural networks and logistic regression grew in popularity. 
Starting from the early 2000s, the popularity of logistic regression and neural networks took off, approaching the popularity of linear regression. Neural networks even surpassed the popularity of linear regression in 2016. All three methods, linear regression, logistic regression, and neural networks are characterized by similarly steep increases in popularity. The overall number of machine learning publications peaked in 2013 (589,803 publications) and has slightly decreased since then (462,045 publications in 2017).

### The competitors: decision trees, SVMs, and Cox regression

The other three, slightly less popular approaches include decision trees, SVMs, and Cox regression. The popularity of these methods is associated with a decidedly slower growth pattern. On the other hand, there also seems to be less fluctuation in the frequency at which these methods are mentioned in the literature. Notably, the popularity of these methods has not declined as much as that of the other methods in the last three years. Among decision trees, SVMs, and Cox regression, SVMs seem to be the most promising method, as their popularity started to exceed that of decision trees only 15 years after their invention.

### Has the neural network bubble burst?

Neural networks have become immensely popular because they have led to breakthroughs in machine learning applications such as image recognition ([ImageNet](http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks), 2012), face recognition ([DeepFace](https://www.cv-foundation.org/openaccess/content_cvpr_2014/html/Taigman_DeepFace_Closing_the_2014_CVPR_paper.html), 2014), and gaming ([AlphaGo](https://ai.googleblog.com/2016/01/alphago-mastering-ancient-game-of-go.html), 2016). In recent years, however, the frequency at which neural networks have been mentioned in scientific articles has decreased. Does this mean that the fascination about neural networks has subsided? 

To answer this question let us consider Figure 2 once again where we can see that the number of mentions of *deep learning*, which is concerned with multilayered neural networks, has drastically increased, while the number of mentions of *neural network* has decreased. The same finding can be made using [Google Trends](https://trends.google.com/trends/explore?date=all&q=deep%20learning,neural%20network). Thus, due to the focus on deep architectures in recent years, it seems that the term *deep learning* has supplanted the term *neural network* to some extent. Figure 3 shows the number of mentions of neural networks when the two terms are pooled.

```{r, echo = FALSE, fig.cap="\\label{fig:models_late_pooled}Overall publication rates of machine learning models from 1980 until now (deep learning and neural networks are pooled)."}
p3
```

The plot suggests that neural networks are as popular as ever. However, for the current year, 2018, we still see a sharp drop in the total mentions of deep learning and neural networks. A likely explanation for this drop is the variety of specialized neural networks that I have not considered in this analysis because they are usually referred to by their acronyms (e.g. convolutional neural networks, CNN; deep belief networks, DBN; deep stacking networks, DSN). Thus, it seems that deep learning research is currently branching out into different specializations rather than distancing itself from neural networks.

## Popularity of supervised learning models across different fields

In the second analysis, I wanted to investigate whether different communities rely on different machine learning techniques. For this purpose, I relied on three repositories for scientific publications: [Google Scholar](https://scholar.google.com) for general publications, [dblp](https://dblp.uni-trier.de/) for computer science publications, and [PubMed](https://www.ncbi.nlm.nih.gov/pubmed/) for publications in the biomedical sciences. In each of the three repositories, I determined [the frequency at which 12 machine learning models were found](https://www.datascienceblog.net/data-sets/ml_models_overall.csv). The results are shown in Figure 4.

```{r, message = FALSE, echo = FALSE, warning = FALSE, fig.cap="\\label{fig:models_across_fields}Rates at which supervised models are mentioned across different fields."}
df <- read.csv("ml_models_overall.csv")
library(reshape2)
df.m <- melt(df, "Source")
# get percentages: values are not comparable otherwise
library(dplyr)
df.m <- group_by(df.m, Source) %>% mutate(percent = value/sum(value, na.rm = TRUE))
colnames(df.m) <- c("Source", "Model", "Count", "Percentage")
# sort columns by overall usage
overall.idx <- which(df.m$Source == "Overall (Google Scholar)")
o <- order(df.m$Count[overall.idx], decreasing = TRUE)
df.m$Model <- factor(gsub("[_.]", " ", df.m$Model), levels = gsub("[_.]", " ", levels(df.m$Model))[o])
ggplot(df.m, aes(x = Model, y = Percentage, fill = Source)) + geom_bar(stat = "identity") +
    theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5)) + 
    scale_y_continuous(labels = scales::percent_format()) +
    ggtitle("Use of machine learning models in different fields")
# top 6 models by source
#n <- 6
#for (source in unique(df.m$Source)) {
    #idx <- which(df.m$Source == source)
    #o <- order(df.m$Percentage[idx], decreasing = TRUE)
    #top.df <- df.m[idx[o[1:n]], ]
    #print(top.df)
#}
```

### Overall use of supervised learning models

According to Google Scholar, these are the five most frequently used supervised models:

1. **Linear regression:** 3,580,000 (34.3%) papers
2. **Logistic regression:** with 2,330,000 (22.3%) papers
3. **Neural networks:** 1,750,000 (16.8%) papers
4. **Decision trees:** 875,000 (8.4%) papers
5. **Support vector machines:** with 684,000 (6.6%) papers

Overall, linear models are clearly dominating, making up more than 50% of all used methods. Non-linear methods are not far behind though: neural networks make second place with 17.1% of all papers, followed by decision trees (8.6% of papers) and SVMs (6.7% of papers).

### Use of models in the biomedical sciences

According to PubMed, the five most popular machine learning models in the biomedical sciences are:

1. **Logistic regression:** 229,956 (54.5%) papers
2. **Linear regression:** 84,850 (20.1%) papers
3. **Cox regression:** 38,801 (9.2%) papers
4. **Neural networks:** 23,883 (5.7%) papers
5. **Poisson regression:** 12,978 (3.1%) papers

Overall, we see an overrepresentation of linear models in biomedical publications: four of of the five most popular methods are linear. This is probably due to two
reasons. First, in medical settings, the number of samples is often too small to allow for fitting complex non-linear models. Second, the ability to interpret the results is critical for medical applications. Since non-linear methods are typically harder to interpret, they are less suited for medical applications because high performance alone is typically not sufficient.

The popularity of logistic regression in the PubMed data is likely due to the large number of publications presenting clinical studies. In these studies, the categorical outcomes (i.e. treatment success) is often analyzed using logistic regression because it is well-suited for interpreting the impact of the features on the outcome. 
Note that that Cox regression is so popular in the PubMed data because it is frequently used for analyzing Kaplan-Meier survival data.

### Use of models in computer science

The five most popular models in computer science bibliography retrieved from dblp are:

1. **Neural networks:** 63,695 (68.3%) papers
2. **Deep learning:** 10,157 (10.9%) papers
3. **Support vector machines:** 7,750 (8.1%) papers
4. **Decision trees:** 4,074 (4.4%) papers
5. **Nearest neighbors:** 3,839 (2.1%) papers

The distribution of machine learning models mentioned in computer science publications is quite distinct: the majority of publications seems to deal with recent, non-linear methods such as neural networks, deep learning, and support vector machines. It is particularly noteworthy that more than three quarters of all retrieved publications deal with neural networks (including deep learning).

### A cleavage between communities

```{r, echo = FALSE, warning = FALSE, fig.cap="\\label{fig:models_fields_parametric}Types of supervised learning methods used in different fields."}
###
# group parametric and non-parametric methods
###
# param.methods: parametric and semi-parametric (e.g. cox)
param.methods <- c("Linear Regression", "Logistic Regression", "Decision Tree", "Cox Regression", 
                    "Poisson Regression", "Linear Discriminant Analysis", "Log Linear Model", 
                    "Ridge Regression", "Lasso Regression")
non.param.methods <- c("Neural Network", "Support Vector Machine", "Deep Learning",
                        "Nearest Neighbor", "Random Forest")
df.m$Class <- ifelse(df.m$Model %in% param.methods, "Parametric", "Non-Parametric")
p.parametric <- ggplot(df.m, aes(x = Source, y = Percentage, fill = Class)) + geom_bar(stat = "identity") +
    theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5)) + 
    scale_y_continuous(labels = scales::percent_format()) +
    ggtitle("Use of machine learning models in different fields")
p.parametric
# total percentages
percentage.df <- ddply(df.m, .(Source, Class), summarize, Total = sum(Percentage))
```

Figure 5 summarizes the percentage of parametric (including semi-parametric) and non-parametric models that are mentioned in the literature. The bar plot demonstrates that there is a big difference between the models investigated in machine learning research (as evidenced by computer science publications) and the types of models that are applied (as evidenced by biomedical and overall publications). While more than 90% of computer science publications deal with non-parametric models, roughly 90% of biomedical publications deal with parametric models. This showcases that machine learning research is heavily focused on state-of-the-art methods such as deep neural networks, while users of machine learning often rely on more interpretable, parametric models.
