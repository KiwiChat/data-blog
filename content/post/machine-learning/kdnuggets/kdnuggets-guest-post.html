---
title: "KDNuggets Guest Post: Machine Learning Through the Years"
author: Matthias Döring
date: '2018-11-26'
description: "TODO"
draft: true
categories:
  - machine-learning
---



<p>The field of machine learning has gone through enormous changes in the last decades. For example, support vector machines and random forests were developed in the mid 1990s. However, some methods that are still used nowadays have been around for quite a long time. For example, the concept of least squares was already proposed in the early 19th century by Legendre and Gauss. Other approaches such as neural networks, which have been described already in the 1950s, have been steadily advanced through the years.</p>
<div id="machine-learning-models-through-the-years" class="section level2">
<h2>Machine learning models through the years</h2>
<p>A question that is often asked is: what is the best model? Answering this question is not really possible as the choice of the model is inherently linked to the data that is being modeled. A question that is easier to answer is: what is the most popular model? Here, I investigate this question in a data-driven manner. More precisely, I will define popularity in a frequentist manner. The assumption of this analysis is that a method that is associated with a greater number of
publications is more popular. Obviously, this is not necessarily true because publications can also indicate that an algorithm is criticized or that there is a lot of room for improvement. However, overall, there should be a high correlation between the two concepts.</p>
<p>For this piece, I heavily relied on data scraping from Google Scholar. Since Google blocks automated requests, I found <a href="https://www.scrapehero.com/how-to-fake-and-rotate-user-agents-using-python-3/">helpful advice from ScrapeHero</a> to circumvent this problem.</p>
<pre class="r"><code>df &lt;- read.csv(&quot;result.csv&quot;)
# select only the top-k models overall (from 1950 to now)
k &lt;- 5
library(plyr)
total.df &lt;- ddply(df, &quot;Model&quot;, summarize, Total = sum(Count))
o &lt;- order(total.df$Total, decreasing = TRUE)
top.models &lt;- total.df[o, &quot;Model&quot;][1:k]
plot.df &lt;- df[df$Model %in% top.models, ]
library(ggplot2)
ggplot(plot.df, aes(x = Year, y = Count, color = Model)) + geom_point() + geom_line()</code></pre>
<p><img src="/post/machine-learning/kdnuggets/kdnuggets-guest-post_files/figure-html/unnamed-chunk-1-1.png" width="672" />
## Popularity of supervised learning models: overall and across different fields</p>
<p>Another question I was interested in was whether there was a difference in the frequencies with which different machine learning models are mentioned in publications from different communities. For this purpose, I counted the number of search results from Google Scholar to determine the overall frequency. For community-specfic frequencies, I performed the same queries on dplb (computer science) and on PubMed (biomedical sciences).</p>
<pre class="r"><code>df &lt;- read.csv(&quot;ml_model_publications.csv&quot;)
library(reshape2)
df.m &lt;- melt(df, &quot;Source&quot;)
# get percentages: values are not comparable otherwise
library(dplyr)</code></pre>
<pre><code>## 
## Attaching package: &#39;dplyr&#39;</code></pre>
<pre><code>## The following objects are masked from &#39;package:plyr&#39;:
## 
##     arrange, count, desc, failwith, id, mutate, rename, summarise,
##     summarize</code></pre>
<pre><code>## The following objects are masked from &#39;package:stats&#39;:
## 
##     filter, lag</code></pre>
<pre><code>## The following objects are masked from &#39;package:base&#39;:
## 
##     intersect, setdiff, setequal, union</code></pre>
<pre class="r"><code>df.m &lt;- group_by(df.m, Source) %&gt;% mutate(percent = value/sum(value, na.rm = TRUE))
colnames(df.m) &lt;- c(&quot;Source&quot;, &quot;Model&quot;, &quot;Count&quot;, &quot;Percentage&quot;)
# sort columns by overall usage
overall.idx &lt;- which(df.m$Source == &quot;Overall (Google Scholar)&quot;)
o &lt;- order(df.m$Count[overall.idx], decreasing = TRUE)
df.m$Model &lt;- factor(df.m$Model, levels = levels(df.m$Model)[o])
ggplot(df.m, aes(x = Model, y = Percentage, fill = Source)) + geom_bar(stat = &quot;identity&quot;) +
    theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5)) + 
    scale_y_continuous(labels = scales::percent_format())</code></pre>
<pre><code>## Warning: Removed 1 rows containing missing values (position_stack).</code></pre>
<p><img src="/post/machine-learning/kdnuggets/kdnuggets-guest-post_files/figure-html/unnamed-chunk-2-1.png" width="672" /></p>
<pre class="r"><code># top 6 models by source
n &lt;- 6
for (source in unique(df.m$Source)) {
    idx &lt;- which(df.m$Source == source)
    o &lt;- order(df.m$Percentage[idx], decreasing = TRUE)
    top.df &lt;- df.m[idx[o[1:n]], ]
    print(top.df)
}</code></pre>
<pre><code>## # A tibble: 6 x 4
## # Groups:   Source [1]
##   Source              Model                   Count Percentage
##   &lt;fct&gt;               &lt;fct&gt;                   &lt;int&gt;      &lt;dbl&gt;
## 1 Biomedical (PubMed) Logistic_Regression    229956     0.542 
## 2 Biomedical (PubMed) Linear_Regression       84850     0.200 
## 3 Biomedical (PubMed) Cox_Regression          38801     0.0914
## 4 Biomedical (PubMed) Neural_Network          23883     0.0563
## 5 Biomedical (PubMed) Poisson_Regression      12978     0.0306
## 6 Biomedical (PubMed) Support_Vector_Machine  11061     0.0261
## # A tibble: 6 x 4
## # Groups:   Source [1]
##   Source                   Model                    Count Percentage
##   &lt;fct&gt;                    &lt;fct&gt;                    &lt;int&gt;      &lt;dbl&gt;
## 1 Overall (Google Scholar) Linear_Regression      3580000     0.304 
## 2 Overall (Google Scholar) Logistic_Regression    2330000     0.198 
## 3 Overall (Google Scholar) Neural_Network         1750000     0.148 
## 4 Overall (Google Scholar) Nearest_Neighbor       1738000     0.147 
## 5 Overall (Google Scholar) Decision_Tree           875000     0.0742
## 6 Overall (Google Scholar) Support_Vector_Machine  684000     0.0580
## # A tibble: 6 x 4
## # Groups:   Source [1]
##   Source                  Model                  Count Percentage
##   &lt;fct&gt;                   &lt;fct&gt;                  &lt;int&gt;      &lt;dbl&gt;
## 1 Computer Science (DBLP) Neural_Network         63695     0.750 
## 2 Computer Science (DBLP) Support_Vector_Machine  7570     0.0891
## 3 Computer Science (DBLP) Decision_Tree           4074     0.0480
## 4 Computer Science (DBLP) Nearest_Neighbor        3839     0.0452
## 5 Computer Science (DBLP) Random_Forest           1863     0.0219
## 6 Computer Science (DBLP) Linear_Regression       1643     0.0193</code></pre>
<p>Overall (according to Google Scholar), these are the 6 most frequently referenced supervised models (from a total of 11,788,100 papers mentioning the considered models):</p>
<ol style="list-style-type: decimal">
<li>Linear regression with 3,580,000 (30.4%) papers</li>
<li>Logistic regression with 2,330,000 (19.8%) papers</li>
<li>Neural networks with 1,750,000 (14.8%) papers</li>
<li>Nearest neighbors with 1,738,000 (14.7%) papers</li>
<li>Decision trees with 875,000 (7.4%) papers</li>
<li>Support vector machines with 684,000 (5.8%) papers</li>
</ol>
<p>The overrepresentation of methods such as linear regression and logistic regression is not surprising because they have been around for a much longer time than other methods. Although support vector machines (SVMs) are one of the more recent methods, they are more popular than other methods that have been developed at the same time (e.g. random forests).</p>
<p>Looking at the numbers in the biomedical and computer sciences, there are critical differences.</p>
<p>Biomedical:</p>
<ol style="list-style-type: decimal">
<li>Logistic regression: 229,956 (54.2%) papers</li>
<li>Linear regression: 84,850 (20%) papers</li>
<li>Cox regression: 38,801 (9.1%) papers</li>
<li>Neural networks: 23,883 (5.6%) papers</li>
<li>Poisson regression: 12,978 (3.1%) papers</li>
<li>Support vector machines: 11,061 (2.6%) papers</li>
</ol>
<p>Computer science:</p>
<ol style="list-style-type: decimal">
<li>Neural networks: 63,695 (75%) papers</li>
<li>Support vector machines: 7,750 (8.9%) papers</li>
<li>Decision trees: 4,074 (4.8%) papers</li>
<li>Nearest neighbors: 3,839 (4.5%) papers</li>
<li>Random forests: 1,863 (2.2%) papers</li>
<li>Linear regression: 1,643 (1.9%) papers</li>
</ol>
<p>Where do these considerable differences come from? The popularity of logistic regression in the PubMed data probably originates from the large number of clinical studies that are contained in the data. There, the categorical outcomes (e.g. treatment success) is often analyzed using logistic regression because it can be used for interpreting the impact of the features on the outcome. In the biomedical data, we see an overrepresentation of non-linear models. This is probably due to two
reasons. First, in medical settings, the number of samples is often small such that it is infeasible to obtain good fits with complex models. Second, interpreting the results is critical for medical applications, which is why linear models are advantageous over non-linear methods, which are typically harder to interpret. It is also remarkable that Cox regression, a model for Kaplan-Meier survival data, is so frequently used (9.1% of papers).</p>
<p>The computer science publications show an opposing trend: the majority of publications concerns recent, non-linear methods. Particularly the number of publications about neural networks, which make up three quarters of all retrieved publications, is astounding. Although SVMs are the second most frequently mentioned method, they are far behind (8.9% of papers). In some way, this is not surprising considering that there have been much more advances in neural networks than in SVMs,
which is likely a byproduct of a greater focus on researching neural networks.</p>
<p>These data also demonstrate why there is such a disconnect between machine learning researchers and persons that apply machine learning. Research is heavily focused on individual, state-of-the-art methods such as convolutional neural networks. Appliers of machine learning, however, still heavily rely on more traditional approaches such as linear or logistic regression.</p>
<p>–
References:
A.M. Legendre. Nouvelles méthodes pour la détermination des orbites des comètes, Firmin Didot, Paris, 1805. “Sur la Méthode des moindres quarrés” appears as an appendix.
C.F. Gauss. Theoria Motus Corporum Coelestium in Sectionibus Conicis Solem Ambientum. (1809)</p>
</div>
