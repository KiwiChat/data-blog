---
title: "Performance Measures for Machine Learning"
author: Matthias DÃ¶ring
date: '2018-11-19'
description: "One of the main criteria of machine learning models is their predictive performance. However, suitable preformances measures differ depending on the specific prediction tasks. This post investigates the most common measures for regression and classification."
categories:
  - machine-learning
---
There are several performance measures for describing the quality of a machine learning model. 

## Performance measures for regression

### The coefficient of determination

For regression, the coefficient of determination, $R^2$, is the performance metric of choice. This measure is based on Pearson's correlation coefficient. Let $\hat{Y}$ indicate the model estimates and $Y$ the observed outcomes. Then, the correlation coefficient is defined as

\[\rho_{\hat{Y}, Y} = \frac{\text{Cov}(\hat{Y}, Y)}{\sigma_{\hat{Y}} \sigma_Y} \]

where $\text{Cov}(\cdot,\cdot) \in \mathbb{R}$ is the covariance and $\sigma$ is the standard deviation. The covariance is defined as

\[\text{Cov}(\hat{Y}, Y) = E[ (\hat{Y} - \mu_{\hat{Y}}) (Y - \mu_Y)] \]

where $\mu$ indicates the mean. In discrete settings, this can be computed as

\[\text{Cov}(\hat{Y}, Y) = \sum_{i=1}^N (\hat{y}_i - \bar{\hat{y}}) (y_i - \bar{y})\,.\]

This means that the coveriance of predictions and outcomes will be positive if they exhibit similar deviations from the mean and negative if they exhibit contrasting deviations from the mean. 

The standard deviation is defined as

\[\sigma_Y = \sqrt{\text{Var}(Y)} = \sqrt{E[(Y - \mu_Y)^2}]\,,\]

which, in discrete settings, can be computed as

\[\sigma_Y = \sqrt{\frac{1}{N} \sum_{i = 1}^N (y_i - \bar{y})^2}\,. \]

Note that the R function ```sd``` computes the population standard deviation, which uses $\frac{1}{N-1}$ to obtain an unbiased estimator. $\sigma$ is high if the distribution is wide (wide spread around the mean) and $\sigma$ is small if the distribution is narrow (little spread around the mean).

### Intuition for correlations: covariance and standard deviations

To understand covariance better, we create a function that plots the deviation of measurements from the mean:

```{r}
plot.mean.deviation <- function(y, y.hat, label) {
    means <- c(mean(y), mean(y.hat))
    y.deviation <- y - mean(y)
    y.hat.deviation <- y.hat - mean(y.hat)
    prod <- y.deviation * y.hat.deviation
    df <- data.frame("N" = c(seq_along(y), seq_along(y)), 
                     "Deviation" = c(y.deviation, y.hat.deviation),
                     "Variable" = c(rep("Y", length(y)), 
                                   rep("Y_Hat", length(y.hat))))
    pos.neg <- ifelse(sign(prod) >= 0, "Positive", "Negative")
    segment.df <- data.frame("N" = c(seq_along(y), seq_along(y)),
                             "Y" = y.deviation, "Yend" = y.hat.deviation,
                             "Sign" = pos.neg, "Contribution" = prod)
    library(ggplot2)
    covariance <- round(cov(y, y.hat), 2)
    correlation <- round(cor(y, y.hat), 2)
    title <- paste0(label, " (Cov: ", covariance, ", Cor: ", correlation, ")")
    ggplot() + 
        geom_segment(size = 2, data = segment.df, 
                    aes(x = N, xend = N, y = Y, yend = Yend, color = Contribution))  +
        geom_point(data = df, alpha = 0.8, size = 2,
                          aes(x = N, y = Deviation, shape = Variable)) +
            xlab("Measurement i of N") + ylab("Deviation from mean value") +
            ggtitle(title) +
     scale_color_gradient2(mid = "grey60") + 
     scale_shape_manual(values = 15:16, 
        label = c(expression(Y), expression(hat(Y))))
}
```

We then generate data representing three types of covariance: positive covariance, negative covariance, and no covariance:

```{r, fig.height = 10}
# covariance
set.seed(1501)
N <- 50
y <- rnorm(N)
set.seed(1001)
# high covariance: similar spread around mean
y.hat <- y + runif(N, -1, 1)
df.low <- data.frame(Y = y, Y_Hat = y.hat)
p1 <- plot.mean.deviation(y, y.hat, label = "Positive Covariance")
# negative covariance: contrasting spread around mean
y.mean <- mean(y)
noise <- rnorm(N, sd = 0.5)
y.hat <- y - 2 * (y - y.mean) + noise
p2 <- plot.mean.deviation(y, y.hat, "Negative Covariance")
# no covariance
y.hat <- runif(N, -0.1, 0.1)
df.no <- data.frame(Y = y, Y_Hat = y.hat)
p3 <- plot.mean.deviation(y, y.hat, "No Covariance")
library(gridExtra)
grid.arrange(p1, p2, p3, nrow = 3)
```

Note that outliers (high deviations from the mean) have a greater impact on the covariance than values close to the mean. Moreover, note that a covariance close to 0 indicates that there does not seem to be an association between variables in any direction (i.e. individual contribution terms cancel out).

Since the covariance depends on the spread of the data, the absolute covariance between two variables with high standard deviations is typically higher than the absolute covariance between variables with low variance. Let us visualize this property:

```{r}
# high variance data
y <- rnorm(N, sd = 10)
y.hat <- y + runif(N, -10, 10)
plot.mean.deviation(y, y.hat, label = "Positive Covariance")
df.high <- data.frame(Y = y, Y_Hat = y.hat)
```

Thus, the covariance by itself does not allow conclusions about the correlation of two variables. This is why Pearson's correlation coefficient normalizes the covariance by the standard deviations of the two variables. Since this standardizes correlations to the range $[-1,1]$, this makes correlations comparable even though the variables have different variances. 
A value of -1 indicates a perfect negative correlation and a value of 1 indicates a perfect positive correlation, while a value of 0 indicates no correlation. 

### The coefficient of determination

The coefficient of determination, $R^2$, is defined as

\[R^2 = 1- \frac{SS_{\rm {res}}}{SS_{\rm {tot}}}\,\]

where $SS_{\rm{res}} = \sum_{i=1}^N (y_i - \hat{y}_i)^2$ is the residual sum of squares and $SS_{\rm{tot}} = \sum_{i=1}^N (y_i - \bar{y})^2$ is the total sum of squares. 

#### Interpretation in terms of the correlation coefficient

R squared is usually positive since models with an intercept produce predictions $\hat{Y}$ with $SS_{\rm{res}} < SS_{\rm{tot}}$ because the preditions of the model are closer to the outcomes than the mean outcome. So, as long as an intercept is present, the coefficient of determination is
the square of the correlation coefficient:

\[R^2 = \rho_{\hat{Y}, Y}^2\,.\]

#### Interpteration in terms of explained variance

In cases where the total sum of squares decomposes into residual and regression sum of squares, 
$SS_{\text{reg}}=\sum _{i}^N(\hat{y}_{i}-{\bar {y}})^{2}$, such that

\[SS_{\text{res}} + SS_{\text{reg}} = SS_{\text{tot}}\,,\]

then 

\[R^{2}={\frac {SS_{\text{reg}}}{SS_{\text{tot}}}}\,.\]

This means that $R^2$ indicates the ratio of variance that is explained by the model. Therefore, a model with $R^2 = 0.7$ would explain $70\%$ of the variance, leaving $30\%$ of the variance unexplained.

### Intuition for the coefficient of determination

To obtain an intuition about $R^2$, we define the following functions with which we can plot the fit of a linear model. The ideal model would lie on a diagonal through the plot and the residuals are indicated as the deviations from this diagonal.

```{r}
rsquared <- function(test.preds, test.labels) {
    return(round(cor(test.preds, test.labels)^2, 3))
}
plot.linear.model <- function(model, test.preds = NULL, test.labels = NULL, 
                            test.only = FALSE) {
    # ensure that model is interpreted as a GLM
    pred <- model$fitted.values
    obs <- model$model[,1]
    if (test.only) {
        # plot only the results for the test set
        plot.df <- NULL
        plot.res.df <- NULL
    } else {
        plot.df <- data.frame("Prediction" = pred, "Outcome" = obs, 
                                "DataSet" = "training")
        model.residuals <- obs - pred
        plot.res.df <- data.frame("x" = obs, "y" = pred, 
                        "x1" = obs, "y2" = pred + model.residuals,
                        "DataSet" = "training")
    }
    r.squared <- NULL
    if (!is.null(test.preds) && !is.null(test.labels)) {
        # store predicted points: 
        test.df <- data.frame("Prediction" = test.preds, 
                            "Outcome" = test.labels, "DataSet" = "test")
        # store residuals for predictions on the test data
        test.residuals <- test.labels - test.preds
        test.res.df <- data.frame("x" = test.labels, "y" = test.preds,
                        "x1" = test.labels, "y2" = test.preds + test.residuals,
                         "DataSet" = "test")
        # append to existing data
        plot.df <- rbind(plot.df, test.df)
        plot.res.df <- rbind(plot.res.df, test.res.df)
        # annotate model with R^2 value
        r.squared <- rsquared(test.preds, test.labels)
    }
    #######
    library(ggplot2)
    p <- ggplot() + 
        # plot training samples
        geom_point(data = plot.df, 
            aes(x = Outcome, y = Prediction, color = DataSet)) +
        # plot residuals
        geom_segment(data = plot.res.df, alpha = 0.2,
            aes(x = x, y = y, xend = x1, yend = y2, group = DataSet)) +
        # plot optimal regressor
        geom_abline(color = "red", slope = 1)
    if (!is.null(r.squared)) {
        # plot r squared of predictions
        max.val <- max(plot.df$Outcome, plot.df$Prediction)
        x.pos <- 0.2 * max.val
        y.pos <- 0.9 * max.val
        label <- paste0("R^2: ", r.squared)
        p <- p + annotate("text", x = x.pos, y = y.pos, label = label, size = 5)
    }
    return(p)
}
```

For example, compare the following models

```{r}
model <- lm(Y ~ Y_Hat, data = df.low)
plot.linear.model(model) 
model <- lm(Y ~ Y_Hat, data = df.no)
plot.linear.model(model)
```

While the model based on ```df.low``` has a sufficient fit (R squared of `r rsquared(df.low$Y, df.low$Y_Hat)`), ```df.low``` does not fit the data well (R squared of `r rsquared(df.no$Y, df.no$Y_Hat)`). 

### Limitations of R squared

Blindly selecting a model solely based on R squared is usually a bad idea. First, R squared does not necessarily tell us something about goodness of fit. For example, consider data with an exponential distribution:

```{r}
x <- rexp(50,rate=0.005)  # exponential
y <- (x + 2.5)^2 * runif(50, min=0.8, max=2) # non-linear relationship with x
plot(x,y)	
```

Let us compute $R^2$ for a linear model fitted on these data:

```{r}
df <- data.frame("x" = x, "y" = y)
model <- lm(x ~ y, data = df)
print(round(summary(model)$r.squared, 2))
```

As we can see, R squared is very high. Still, the model does not fit well because it does not respect the exponential distribution of the data.

Another property of $R^2$ is that it depends on the value range. This is because the covariance is adjusted by the standard deviations of the variables, which is larger for variables with a greater range. 

```{r}
# wide value range:
x <- seq(1,10,length.out = 100)
y <- 2 + 1.2*x + rnorm(100,0,sd = 0.9)
mod1 <- lm(y ~ x)
mse <- sum((fitted(mod1) - y)^2)/100 
print(summary(mod1)$r.squared)
print(mse)
# narrow value range
x <- seq(1,2,length.out = 100)
y <- 2 + 1.2*x + rnorm(100,0,sd = 0.9)
mod1 <- lm(y ~ x)
mse <- sum((fitted(mod1) - y)^2)/100 
print(summary(mod1)$r.squared)
print(mse)
```

As we can see, even though both models have similar residual sum of squares, the first model has a superior $R^2$. 

### Mean squared error

### Mallow's Cp

Mallow's Cp statistic can be used to assess the fit of linear models fitted using least squares. For Gaussian models, it is identical to the Akaike Information Criterion. Small values of Cp are assigned to models with a good fit. The Cp statistic assigns a value of $p+1$ for an ideal model, where $p$ is the number of independent variables. If  $C_p > p+1$, this means that the model is overspecified (i.e. contains too many variables). Assume that there are $k$ available features and you are evaluating a model with $p$ features, then the Cp statistic is defined as:

\[C_p = \frac{SS_{\rm {res}}{MSE_k}} - N + 2 p \]

where $SS_{\rm {res}}$ is the residual sum of squares from the model with $p$ features and $MSE_k$ is the mean-squared error of the model using all of the $k$ features.

## Performance measures for classification
