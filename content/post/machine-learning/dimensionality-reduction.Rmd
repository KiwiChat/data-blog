---
title: "Dimensionality Reduction for Visualization and Prediction"
author: Matthias Döring
date: '2018-11-14T15:00:00Z'
description: "Dimensionality reductions is primarily used for exploring data and for reducing the feature space in machine learning applications. In this post, I investigate techniques such as PCA to obtain insights from a whiskey data set and show how predictive performance can be improved by reducing the number of features."
draft: true
categories:
  - machine-learning
tags:
    - dimensionality reduction
thumbnail: "/post/machine-learning/whiskey-twilight-zone.png"

---

Dimensionality reduction has two primary use cases: data exploration and machine learning. It is useful for data exploration because dimensionality reduction to few dimensions (e.g. 2 or 3 dimensions) allows for visualizing the samples. Such a visualization can then be used to obtain insights from the data (e.g. detect clusters and identify outliers). For machine learning, dimensionality reduction is useful for reducing the feature space, which can lead to models that generalize better. 

In this post, we will investigate three dimensionality reduction techniques:

* Principal components analysis (PCA): the most popular dimensionality reduction method
* Kernel PCA (KPCA): a variant of PCA that allows for nonlinearity
* t-distributed stochastic neighbor embedding(t-SNE): a recently developed nonlinear dimensionality reduction technique

The difference between these approaches is that PCA outputs a rotation matrix, which can be applied on any other matrix in order to transform the data. Neighborhood-based techniques such as t-SNE, on the other hand, do not enable the transformation of new data.

## Loading a whiskey data set

I have previously used a data set describing the characteristics of whiskeys to [draw radar plots](/post/data-visualization/radar-plot/). Here, we will the whiskey data set to identify how different dimensionality reduction techniques perform on this data set.

```{r, message = FALSE}
library(RCurl)
# load data as character
f <- getURL('https://www.datascienceblog.net/data-sets/whiskies.csv')
# read table from text connection
df <- read.csv(textConnection(f), header=T)
head(df)
# select characterics of the whiskeys
features <- c("Body", "Sweetness", "Smoky",
            "Medicinal", "Tobacco", "Honey",
            "Spicy", "Winey", "Nutty",
            "Malty", "Fruity", "Floral")
feat.df <- df[, c("Distillery", features)]
```

## Assumptions about the results

Before we begin reducing the dimensionality of the data, we should think about what kind of results we would like to obtain. For the whiskey data, we would expect that whiskeys with similar taste profiles are close to each other in the reduced space. 

Since whiskeys from distilleries that are in proximity to another use similar distilling techniques and resources, their whiskeys are also often similar. Thus, a reasonable dimensionality reduction should cluster whiskeys from similar regions. 

To validate this assumption, we are going to test whether the mean expression of whiskey characteristics differ between distilleries from different regions. For this, we will run a MANOVA test:

```{r}
m <- manova(as.matrix(df[, features]) ~ Region, df)
summary(m)
```

The test statistic is significant at the 5% level, so we can reject the null hypothesis (there is no effect of region on the characteristics).

## Geographical locations of the distilleries

Since regionality seems to be an important element, we will explore where the distilleries in the data set are located by ploting their latitude and longitude. The following Scotch whiskey regions exist:

![Scotch regions (Licensed under CC BY-SA 3.0 and retrieved from https://commons.wikimedia.org/wiki/File:Scotch_regions.svg)](https://upload.wikimedia.org/wikipedia/commons/f/fd/Scotch_regions.svg)

Having transformed the `Latitude` and `Longitude` features [from UTM coordinates to degrees](/post/other/whiskey-data-annotation/), we can localize the distilleries:

```{r, message = FALSE}
library(ggplot2) # for map_data and ggplot functions
library(ggrepel) # for geom_text_repel: smart text labels
uk.map <- map_data ("world", region = "UK") 
scotland.map <- uk.map[uk.map$subregion == "Scotland",]
p <- ggplot(data = scotland.map, aes(x = long, y = lat)) + 
geom_map(map = uk.map, 
       aes(map_id = region),
       fill="white", colour = "black") +
coord_map() + 
geom_point(data = df, aes(y = long, x = lat, color = Region),
     alpha = .75) +
ggtitle ("Locations of Scottish Whiskey Distilleries")
# for storing the map with labels:
#geom_text_repel(data = map.df, aes(y = long, x = lat, label = Distillery)) 
# ggsave(file.path("distillery_map.png"), p, units = "cm", height = 80, width = 80)
p
```

I also created a [high-resolution version of the distillery map](/post/machine-learning/distillery_map.png) where the labels of the distilleries are annotated.

## PCA

PCA computes a rotation matrix $W \in \mathbb{R}^{p \times p}$ from the matrix of features $X \in \mathbb{R}^{N \times p}$. $W$ can be understood as a mapping function that transforms the observations in $X$ to a rotated space. The coordinates of observations in $X$ are transformed to their new form, $Z$, via

\[Z = XW\,.\]

The rotation matrix $W$ is constructed through orthogonal linear transformations. Each of these transformations is performed such that it maximizes the variance in the unexplained variance in the data, resulting in individual *principal components*, which provide a new coordinate system.

One of the reasons why PCA is so popular is that it is a very interpretable method. Each principal component (PC) is well-defined as we know that it is a dimension orthogonal to the other dimensions. Since PCA maximizes the variance with each PC, we can obtain the variance that is explained by each PC in order to select an appropriate number of dimensions. 

### Results of PCA

Using PCA, I would assign three clusters to the data:

```{r, message = FALSE, fig.width = 16, fig.height = 16}
library(cluster)
library(ggfortify)
data <- df[,features]
rownames(data) <- paste0(df$Distillery, " (", df$Region, ")")
cl <- pam(data, 3)
# learn about the interpretation of the principal componenets:
autoplot(cl, frame = TRUE, frame.type = 'norm', loadings.label = TRUE,
         loadings.colour = "black", loadings.label.colour = "black")
autoplot(cl, frame = TRUE, frame.type = 'norm', label.repel = TRUE,
         label = TRUE, label.colour = "black", shape = FALSE)
```

The principal components seem to reflect the following characteristics:

* PC1 seems to indicate the *intensity of the taste*: i.e. a smoky, medicinal taste (e.g. Laphroaig or Lagavulin) vs a smoother experience (e.g. Auchentoshan or Abelour)
* PC2 seems to indicate the *complexity of the taste*: i.e. an extremely balanced taste profile (e.g. Glenfiddich or Auchentoshan) vs a more characteristic taste profile (e.g. Glendronach or Macallan)

Let us verify whether the clusters indeed overrepresent certain regions:

```{r}
tabs <- vector("list", 3)
for (i in seq(3)) {
    idx <- which(cl$clustering == i)
    regions <- df$Region[idx]
    tabs[[i]] <- table(regions)
}
cluster.df <- data.frame("Cluster" = 1:3, do.call(rbind, tabs))
print(cluster.df)
```

Indeed, certain regions are overrepresented in the three clusters. In summary, the interpretation of the clusters is as follows:

* **Cluster 1:** *Complex whiskeys*, mostly from the Highlands/Speyside
* **Cluster 2:** *Well-balanced whiskeys*, mostly from Speyside and Highlands
* **Cluster 3:** *Smoky whiskeys*, mainly from Islay

There are some interesting observations to make from the visualization:

* Oban and Clynelish are the only Highlands distilleries that produce tastes resembling those from distilleries on Islay.
* Highland and Speyside whiskeys differ mainly in one dimension. At one extreme are smooth, well-balaned whiskeys such as Glenfiddich. At the other extreme, are whiskeys with a more characteristic tase profile such as Macallan.

## Kernel PCA

KPCA is an extension of PCA that makes use of kernel functions, which are the most prominent characteristc of support vector machines. By mapping the data into a reproducing kernel Hilbert space, it is possible to separate data even if they are not linearly separable. 

In KPCA, observations are transformed to a kernel matrix via

\[K = k(x,y) = \phi(x)^T \phi(y)\]

where $k(x,y)$ is the kernel function for observations $x$ and $y$ and $\phi$ is the corresponding function that maps the observations into reproducing kernel Hilbert space and does not need to be explicitly computed due to the *kernel trick*. According to the kernel trick, we just need to compute the kernel function.

### Using KPCA in R

The ```kpca``` is made available through the ```kernlab``` package. By default, the radial basis function kernel is used for matrix input. It is defined by

\[K (x, y) = \exp \left(- \frac{-||x − t||^2}{2 \sigma^2}\right)\]

where $\sigma$ is the inverse kernel width. 

```{r, message = FALSE}
library(kernlab) # for kpca function
# use the default of sigma = 0.1 because it works well
pca.k <- kpca(as.matrix(data), kpar = list(sigma = 0.1))
plot(pca.k@eig) # eigenvalues
# select 20 dimensions for prediction according to eigenvalues
n.dim.model <- 20
pc <- pca.k@pcv
```

Let us plot the data:

```{r, fig.width = 10, fig.height = 10}
library(ggrepel) # for geom_text_repel
kpca.df <- data.frame("Label" = df$Distillery, "Region" = df$Region,
                      "PC1" = pc[,1], "PC2" = pc[,2])
ggplot(kpca.df, aes(label = Label, x = PC1, y = PC2, color = Region)) + 
    geom_text_repel()
```

In terms of the visualization, the results are similar to what we obtained with conventional PCA. The whiskeys from Islay are well-separated and we can see a cluster of Speyside whiskeys, while the Highlands whiskeys are rather spread throughout.

A disdavantage of KPCA is that you need to deal with the hyperparameters of the kernel functions, which should be tuned in accordance with the data. Moreover, KPCA is not as interpretable as PCA because it is not possible to determine how much variance is explained by individual dimensions.

## t-SNE

t-SNE can be performed in the following way:

```{r, message = FALSE, fig.width = 10, fig.height = 10}
library(Rtsne)
set.seed(1234) # reproducibility
tsne <- Rtsne(data, dims = 2, perplexity = 5)
t.df <- as.data.frame(tsne$Y)
colnames(t.df) <- c("V1", "V2")
t.df <- cbind(t.df, Cluster = factor(cl$clustering))
t.df$Distillery <- rownames(t.df)
ggplot(t.df, aes(x = V1, y = V2, color = Cluster, label = Distillery)) +
    geom_point() + geom_text()
```

The result is a bit more clearer with t-SNE than with PCA and the two dimensions have slightly different interpretations than with PCA:

* V1 seems to indicate how balanced/intense the taste is. The outliers here are the smoky Islay whiskeys on the left (e.g. Lagavulin) as well as some of the richer Highland whiskeys on the right (e.g. Macallan). The whiskeys in the middle such as Glenfiddich have a very balanced taste profile.
* V2: I cannot figure that out.

## Using PCA for machine learning

When validating a machine learning that is fitted to data that has been transformed using PCA, it is crucial that PCA is done independently for the training and test data sets. Why? If PCA were performed on the whole data set, the orthogonal projection obtained via PCA would be influenced by the test data. Thus, when testing the performance of the model on the test data, the performance of the model would be overestimated since the projection is tuned to the space in which the test samples reside. Thus, the following approach needs to be followed:

1. Perform PCA on the test data set and train the model on the transformed data.
2. Apply the learned PCA transformation from the training data on the test data set and evaluate the performance of the model on the transformed data.

To exemplify the workflow, let us predict the region that a whiskey originates from given its taste profile. For this purpose, we will use the $k$-nearest neighbor model because we the few features we have (p = `r length(features)`) will be further reduced by PCA. Moreover, the feature space is small because all variables are in $[0,4]$. Since we have to optimize the value of $k$, we also set aside a validation set for determining this parameter.

### Obtaining the PCA transformation

First, we write some funtions for validating the performance of the prediction. We will simply use the accuracy here, although another performance measure may be more appropriate because it is likely that the regions for which few samples are available are confused more often. Moreover, we assign 50% of the observations to the training set, 25% for the validation set (for tuning $k$), and 25% for testing the performance of the selected model.

```{r}
# split data into 3 parts: training, validation, and testing
get.accuracy <- function(preds, labels) {
    correct.idx <- which(preds == labels)
    accuracy <- length(correct.idx) / length(labels)
    return(accuracy)
}
select.k <- function(K, training.data, test.data, labels, test.labels) {
    performance <- vector("list", length(K))
    for (i in seq_along(K)) {
        k <- K[i]
        preds <- knn(train = training.data, test = test.data, 
                     cl = labels, k = k)
        validation.df <- cbind("Pred" = as.character(preds), "Ref" = as.character(test.labels))
        #print(k)
        #print(validation.df)
        accuracy <- get.accuracy(preds, test.labels)
        performance[[i]] <- accuracy
    }
    # select best performing k
    k.sel <- K[which.max(performance)]
    return(k.sel)
}
set.seed(1234) # reproducibility
samp.train <- sample(nrow(data), nrow(data)*0.50) # 50 % for training
df.train <- data[samp.train,,]
# 25% for validation
samp.test <- sample(setdiff(seq(nrow(data)), samp.train), length(setdiff(seq(nrow(data)), samp.train)) * 0.5)
df.test <- data[samp.test,]
samp.val <- setdiff(seq_len(nrow(data)), c(samp.train, samp.test))
df.val <- data[samp.val, ]
```

Having prepared the evaluation functions and the data, we can transform the data using PCA. Note that the PCA is only run on the training data and not on the other data sets. Rather, we use the ```predict.princomp``` function to apply the rotation matrix obtained from the training data onto the other data sets. Note that the name of this function is misleading because it does not really predict anything.

```{r}
# PCA on training data
# NB: scale is FALSE since all variables are on the same scale
pca <- prcomp(df.train, retx=TRUE, center = TRUE, scale = FALSE) 
# find explained variance per PC:
expl.var <- round(pca$sdev^2/sum(pca$sdev^2)*100)
var.df <- data.frame("N_dim" = seq_along(expl.var), "Cum_Var" = cumsum(expl.var))
# cumulative explained variance:
print(t(var.df))
# select 3 dimensions (> 50% variance is sufficient)
n.dims <- 3
# transform all data using PCA projection from training data
# NB: predict.princomp(pca, newdata) <=> as.matrix(newdata) %*% pca$rotation
df.train.p <- predict(pca, newdata = df.train)[, 1:n.dims]
df.val.p <- predict(pca, newdata = df.val)[, 1:n.dims]
df.test.p <- predict(pca, newdata = df.test)[, 1:n.dims]
```

Now that we have transformed the training, validation, and test sets into PCA space, we can use $k$-nearest neighbors. Note that this prediction scenario is challenging because some regions such as Islands and Lowlands are underrepresented. If we would select $k$ with a very large value (e.g. 30), then most samples would be assigned to the overrepresented regions. Since we are using accuracy as a performance measure, such a classifier would unfortunately actually perform well. Thus, we conservatively limit the range of $k$ in order to prevent this from happening.

```{r}
# train k-nearest neighbor models to find ideal value of k
library(class) # for knn classifier
K <- 3:10 # conservative number of nearest neighbors to consider 
k.sel.pca <- select.k(K, df.train.p, df.val.p, df[samp.train, "Region"], 
                      df[samp.val, "Region"])
# determine performance on test set
test.preds.pca <- knn(train = df.train.p, test = df.test.p, 
                      cl = df$Region[samp.train], k = k.sel.pca)
accuracy.pca.knn <- get.accuracy(test.preds.pca, df[samp.test, "Region"])
print(paste0("PCA+KNN accuracy for k = ", k.sel.pca, " is: ", 
             round(accuracy.pca.knn, 3)))
```

Let us investigate whether the model that uses PCA outperforms the model based on the raw data:

```{r}
# compare with accuracy of non-PCA model
k.sel <- select.k(K, df.train, df.val, df[samp.train, "Region"], 
                  df[samp.val, "Region"])
test.preds <- knn(train = df.train, test = df.test, 
                  cl = df$Region[samp.train], k = k.sel)
accuracy.knn <- get.accuracy(test.preds, df[samp.test, "Region"])
print(paste0("KNN accuracy for k = ", k.sel, " is: ",
             round(accuracy.knn, 3)))
```

So, using $k$-nearest neighbors, PCA does indeem seem to boost the prediction accuracy for this data set although there are few features to begin with. However, there are some low-variance features in the data set (e.g. ```Tobacco``` or ```Malty```):

```{r}
# variances of whiskeys characteristics
print(diag(var(data)))
```

Now that we are able to identify the six regions of Scottish whiskey with a respectable accuracy only by their taste, the question is if we can still obtain a better performane. We know that it is hard to predict the Scotch regions that are underrepresented in the data set. So, what would happen if we limit ourselves to fewer regions? The PCA analysis suggest that we could regroup the labels in the following way:

* Island whiskeys are grouped with Islay whiskeys 
* Lowland/Campbeltown whiskeys are grouped with Highland whiskeys

In this way, the problem is reduced to three regions: Island/Islay whiskeys, Highland/Lowland/Campbeltown whiskeys, and Speyside whiskeys. Let us run the analysis again:

```{r}
# regroup labels
labels <- df$Region
labels[which(labels == "Islands")] <- "Islay"
labels[which(labels == "Lowlands")] <- "Highlands"
labels[which(labels == "Campbeltown")] <- "Highlands"
# rename groups
labels <- factor(labels)
levels(labels) <- c("Highlands/Lowlands/Campbeltown", "Islay/Islands", "Speyside")
# increase range for k: we have more samples per region now
k.sel.pca <- select.k(3:20, df.train.p, df.val.p, labels[samp.train],
                      labels[samp.val])
test.preds.pca <- knn(train = df.train.p, test = df.test.p, 
                      cl = labels[samp.train], k = k.sel.pca)
accuracy.pca.knn <- get.accuracy(test.preds.pca, labels[samp.test])
print(paste0("PCA+KNN accuracy for k = ", k.sel.pca, " is: ", 
             round(accuracy.pca.knn, 3)))
```

With an accuracy of `r paste0(round(accuracy.pca.knn, 3)*100, "%")`, we can conclude that, it is indeed worthwhile to group the whiskey regions for which we have fewer samples. 

### KPCA for Prediction

Applying KPCA for prediction is not as straight-forward as applying PCA. In PCA, the eigentvector are computed in the input space but in KPCA, the eigenvectors come from kernel Hilbert space. Thus, it is not simply possible to transform new data points when we do not know the explicit mapping function $\phi$ that is used. 

What is easily possible is to create a model from the transformed data. However, this approach is not helpful for doing validation because this would mean that we include the test set in the PCA. So, the approach in the following approach **should not be used for validating a model**:

```{r}
Z <- pca.k@rotated[,1:20] # the transformed input matrix
preds.kpca <- knn(train = Z[samp.train,], test = Z[samp.test,], 
                     cl = df$Region[samp.train], k = k.sel.pca)
# NB: this would overestimate the actual performance
accuracy <- get.accuracy(preds.kpca, df$Region[samp.test])
```

Besides this property, KPCA may not always actually reduce the number of features. This is because the kernel functions actually lead to an increase in the number of parameters. Thus, in some instances, it may not be possible to find a projection with fewer dimensions than initially.

## Summary

TODO


## Outlook

I find it interesting that the two-dimensional projection of whiskeys has so many large empty areas. This could indicate one of two things:

1. There is still a lot of potential for experimenting with new, exciting types of whiskeys
2. There are just so many taste combinations that go well.

I am inclined to go with the second option. Why? In the PCA plot, the lower right is the largest region in which no samples reside. Looking at the whiskeys that come close to this region, we find that those are Macallan on the y-axis and Lagavulin on the x-axis. Macallan is known for its complex taste and Lagavulin is known for its smoky taste. So, a whiskey that comes to lie on the lower right of the 2-dimensional PCA space would have both properties: a complex taste plus a very smoky taste. I would assume that both tastes would be just too much for the palate to handle (i.e. smokiness masks complexity). Thus, this unexplored region of taste can be considered to be *the whiskey twilight zone*, as exemplified by this plot:

![Whiskey twilight zone](https://www.datascienceblog.net/post/machine-learning/whiskey-twilight-zone.png)

