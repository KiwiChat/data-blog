---
title: "Effect Sizes: Why Significance Alone is Not Enough"
author: Matthias Döring
date: '2018-10-20'
thumbnail: "post/statistical_test/effect_size_cover.png"
categories:
  - statistical test
tags:
  - effect size
  - significance
  - Cramer's V
  - chi-squared test
---



<p>So, you performed a test for significance and you obtained a positive result. That’s great but it’s not yet time to celebrate. You may ask: <em>Why? Isn’t a significant test sufficient to show the existence of an effect?</em> This, however, is only true to some extent. First, a significant result never proves the existence of an effect. For example, at a significance level of 5%, an exact test will yield a false positive result in 5% of the cases. Second, a significance test may
fail to detect an effect if the sample size is too small; this means that the test is not sufficiently powered to detect the effect.
Third, a significant test does not necessarily make a statement about the magnitude of an effect. In this post, we’ll investigate the difference between statistical significance and effect size.</p>
<!--more-->
<div id="significance-depends-on-sample-size-and-effect-size" class="section level2">
<h2>Significance depends on sample size and effect size</h2>
<p>To exemplify the difference between statistical significance and effect size, let’s assume that we are conducting a study investigating two groups, <span class="math inline">\(G_1\)</span> and <span class="math inline">\(G_2\)</span>, with respect to two outcomes, <span class="math inline">\(Y_1\)</span> and <span class="math inline">\(Y_1\)</span>.</p>
<div id="smallest-sample-size-largest-effect" class="section level3">
<h3>Smallest sample size, largest effect</h3>
<p>Assume that we didn’t have enough funds, so we ended up with only 40 measurements that are equally split between groups 1 and 2. We are first going to define a function that builds contingency tables with the following properties:</p>
<ul>
<li>The number of samples is <code>sample.size</code></li>
<li>Each group contains 50% of the measurements</li>
<li>The relative difference between the frequencies of the two groups is <code>diff</code></li>
</ul>
<pre class="r"><code>build.contingency.table &lt;- function(sample.size, diff) {
    g.size &lt;- sample.size / 2 # group size
    data &lt;- matrix(c(g.size/2 - (diff * g.size/2), g.size/2 + (diff * g.size/2), 
                   g.size/2, g.size/2),
                   nrow = 2, byrow = TRUE,
                   dimnames = list(c(&quot;G1&quot;, &quot;G2&quot;), c(&quot;Y1&quot;, &quot;Y2&quot;)))
    return(data.frame(data))
}</code></pre>
<p>Let’s assume the difference between the populations is 60%. This means that <span class="math inline">\(0.6 \cdot 20 = 12\)</span> samples in <span class="math inline">\(G_1\)</span> will have a different outcome than in <span class="math inline">\(G_2\)</span>. Under this assumption, we obtain the following contingency table and test whether the frequencies are independent of the groups using the <span class="math inline">\(\chi^2\)</span> test:</p>
<pre class="r"><code>data.L &lt;- build.contingency.table(40, 0.6)
print(data.L)</code></pre>
<pre><code>##    Y1 Y2
## G1  4 16
## G2 10 10</code></pre>
<pre class="r"><code>chi.result.L &lt;- chisq.test(data.L)
print(chi.result.L$p.value)</code></pre>
<pre><code>## [1] 0.09742169</code></pre>
<p>Interestingly, the p-value of 0.0974 is not significant at the 5% despite the large difference between the two groups.</p>
</div>
<div id="medium-sample-size-medium-effect" class="section level3">
<h3>Medium sample size, medium effect</h3>
<p>Let’s now assume that we’ve collected a greater number of samples than in the first study, namely a total of <span class="math inline">\(10\,000\)</span> samples. This time, however, the effect size is smaller: the difference between the two groups is only <span class="math inline">\(\frac{200}{5000} = 4\%\)</span>. If we are testing whether the outcome is independent of the grouping, what do you think, will a <span class="math inline">\(\chi^2\)</span> test on the corresponding contingency table be insignificant? Let’s see the result:</p>
<pre class="r"><code>data.M &lt;- build.contingency.table(10000, 0.04)
print(data.M)</code></pre>
<pre><code>##      Y1   Y2
## G1 2400 2600
## G2 2500 2500</code></pre>
<pre class="r"><code>chi.result.M &lt;- chisq.test(data.M)
print(chi.result.M$p.value)</code></pre>
<pre><code>## [1] 0.04765904</code></pre>
<p>Indeed, the p-value of 0.0477 is sufficiently small for a significant result at the 5% level. This could come as a surprise considering that the outcomes were more similarly split across both groups than in the first experiment.</p>
</div>
<div id="largest-sample-size-smallest-effect" class="section level3">
<h3>Largest sample size, smallest effect</h3>
<p>To illustrate this point, let’s generate a data set with an even larger sample size and a smaller difference between the groups. We’ll take a sample of 1 million and artificially enforce a difference between the groups of only <span class="math inline">\(1\%\)</span>:</p>
<pre class="r"><code>data.S &lt;- build.contingency.table(1000000, 0.01)
print(data.S)</code></pre>
<pre><code>##        Y1     Y2
## G1 247500 252500
## G2 250000 250000</code></pre>
<pre class="r"><code>chi.result.S &lt;- chisq.test(data.S)
print(chi.result.S$p.value)</code></pre>
<pre><code>## [1] 5.790922e-07</code></pre>
<p>In this case, the p-value is even smaller than for the previous data set although the difference between the two groups has been reduced from 4% to 1%.</p>
<p>How can these results be explained? We have to remember that the p-value indicates how likely it is to obtain a test result that is at least as extreme by chance. Since chance is reduced with greater sample sizes, it becomes easier to show differences between groups (i.e. the statistical power increases). Therefore, at large sample sizes, even small effects can become significant, while for small sample sizes, even large effects may not be significant.</p>
</div>
</div>
<div id="determining-the-effect-size-with-cramers-v" class="section level2">
<h2>Determining the effect size with Cramer’s V</h2>
<p>The effect size of the <span class="math inline">\(\chi^2\)</span> test can be determined using Cramer’s V. Cramer’s V is a normalized version of the <span class="math inline">\(\chi^2\)</span> test statistic. It is defined by
<span class="math display">\[V = \sqrt{\frac{\chi^2}{n \cdot (c - 1)}}\]</span>
where <span class="math inline">\(n\)</span> is the sample size and <span class="math inline">\(c = \min(m,n)\)</span> is the minimum of the number of rows <span class="math inline">\(m\)</span> and columns <span class="math inline">\(n\)</span> in the contingency table. Interpretation of Cramer’s V is easy due to <span class="math inline">\(V \in [0,1]\)</span>. For large effects, <span class="math inline">\(V\)</span> will approach 1 but if there’s no effect <span class="math inline">\(V\)</span> will be close to 0.</p>
<div id="function-for-computing-cramers-v" class="section level3">
<h3>Function for computing Cramer’s V</h3>
<p>Since there’s no function to compute Cramer’s V in base R, we’ll implement it ourselves as follows:</p>
<pre class="r"><code>cramer.v &lt;- function(contingency.tab) {
    chi &lt;-  chisq.test(contingency.tab, correct = FALSE)$statistic
    n &lt;- sum(contingency.tab)
    c &lt;- min(nrow(contingency.tab), ncol(contingency.tab))
    V &lt;- sqrt(chi / (n * (c-1)))
    return(as.numeric(V))
}</code></pre>
</div>
<div id="cramers-v-for-the-three-data-sets" class="section level3">
<h3>Cramer’s V for the three data sets</h3>
<p>We will determine Cramer’s V for the three data sets <code>data.L</code>, <code>data.M</code>, and <code>data.S</code>, which exhibit large, medium, and small effect sizes, respectively.</p>
<pre class="r"><code>data &lt;- list(&quot;Largest_Effect&quot; = data.L, 
             &quot;Medium_Effect&quot; = data.M, 
             &quot;Smallest_Effect&quot; = data.S)
Vs &lt;- sapply(data, cramer.v)
print(Vs)</code></pre>
<pre><code>##  Largest_Effect   Medium_Effect Smallest_Effect 
##     0.314485451     0.020004001     0.005000063</code></pre>
<p>The obtained values for Cramer’s V agrees well with with the expectation. To interpret Cramer’s V, the following approach is often used:</p>
<ul>
<li><span class="math inline">\(V \in [0.1, 0.3]\)</span>: weak association</li>
<li><span class="math inline">\(V \in [0.4, 0.5]\)</span>: medium association</li>
<li><span class="math inline">\(V &gt; 0.5\)</span>: strong association</li>
</ul>
<p>Thus, for the three data sets we analyzed, only the first data set exhibited a weak association, while the other two data sets, albeit significant, exhibited no association according to this interpretation of Cramer’s V.</p>
</div>
</div>
<div id="conclusions" class="section level2">
<h2>Conclusions</h2>
<p>If you are reporting the significance of a test, it is similarly important to report the effect size in order to document the magnitude of the effect. The available measures for the effect size depend on the significance test. For example, for the <span class="math inline">\(\chi^2\)</span> test, the effect size can be determined using Cramer’s V.</p>
<p>What importance do you assign to effect sizes? Do you think that they are often overlooked?</p>
</div>
