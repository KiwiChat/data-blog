---
title: Signed Wilcoxon Rank Test
author: Matthias Döring
date: '2018-10-16'
thumbnail: "post/statistical_test/signed_wilcox_rank_test_cover.png"
categories:
  - statistical test
tags:
  - Wilcoxon
  - non-parametric test
  - statistical test
  - box plot
  - one-tailed
  - paired test
---



<p>In this post, we will investigate the use of the signed Wilcoxon rank test. This is a non-parametric test that is appropriate for comparing continuous measurements from two groups. Using the test, it is possible to determine whether the observations have a distinct ordering that depends on the observed grouping. <!--more--></p>
<div id="the-sleep-data-set" class="section level2">
<h2>The sleep data set</h2>
<p>Let’s consider the sleep data set. The data set contrasts the effect of two soporific drugs (aka sleeping pills) by providing the change in the number of hours slept after taking the drug compared to a baseline:</p>
<pre class="r"><code>data(sleep) # load the sleep data set
print(sleep)</code></pre>
<pre><code>##    extra group ID
## 1    0.7     1  1
## 2   -1.6     1  2
## 3   -0.2     1  3
## 4   -1.2     1  4
## 5   -0.1     1  5
## 6    3.4     1  6
## 7    3.7     1  7
## 8    0.8     1  8
## 9    0.0     1  9
## 10   2.0     1 10
## 11   1.9     2  1
## 12   0.8     2  2
## 13   1.1     2  3
## 14   0.1     2  4
## 15  -0.1     2  5
## 16   4.4     2  6
## 17   5.5     2  7
## 18   1.6     2  8
## 19   4.6     2  9
## 20   3.4     2 10</code></pre>
<p><em>extra</em> indicates the increase/decrease (positive/negative values) in sleep compared to the baseline measurement, <em>group</em> denotes the drug, and <em>ID</em> gives the patient ID. To make this clearer, I’ll rename <em>group</em> to <em>drug</em>:</p>
<pre class="r"><code>colnames(sleep)[which(colnames(sleep) == &quot;group&quot;)] &lt;- &quot;drug&quot;</code></pre>
</div>
<div id="what-are-we-testing-for" class="section level2">
<h2>What are we testing for?</h2>
<p>Assume we are working in a pharmaceutical company and these are the data that were just obtained from a clinical trial. Now, we must decide which of the two drugs you should go forward with for market release. A reasonable way of selecting the drug would be to identify the drug that performs better. More specifically, the question is the following: is one of the drugs associated with greater values of <em>extra</em> than the other drug?</p>
<p>To get an intuition about the effectiveness of the two drugs, let’s plot their corresponding values:</p>
<p><img src="/post/statistical_test/signed_wilcox_rank_test_files/figure-html/unnamed-chunk-3-1.png" width="672" /></p>
<p>The plot shows that the median increase in sleep time of drug 1 is close to 0, while the median increase for drug 2 is close to 2 hours. So, based on these data it seems that drug 2 is more effective than drug 1. However, we still need to determine whether our finding is statistically significant.</p>
<div id="the-null-hypothesis" class="section level4">
<h4>The null hypothesis</h4>
<p>The null hypothesis of the test is that there isn’t any difference in the extra sleep time between the two drugs. Since we want to find out whether drug 2 outperforms drug 1, we do not need a two-tailed test (testing whether any of the drugs has superior performance), but a one-tailed test. Thus, the alternative of the null hypothesis is that drug 2 is associated with greater values of <em>extra</em> than drug 1.</p>
</div>
</div>
<div id="specifying-the-test" class="section level2">
<h2>Specifying the test</h2>
<p>Since we have two measurements for every patient, we need a <strong>paired test</strong>. In this case, the test is called <em>signed Wilcoxon rank test</em> because it uses the sum of the signed ranks as the test statistic. Since <em>wilcox.test</em> does not consider pairs by default, we have to explicitly set the <em>paired</em> argument to perform the test. To specify the one-tailed test, we set the <em>alternative</em> argument to <em>greater</em>. In this way, the alternative of the test is whether drug 2 is associated with larger increases in sleep duration than drug 1.</p>
<pre class="r"><code>x &lt;- sleep$extra[sleep$drug == 2]
y &lt;- sleep$extra[sleep$drug == 1]
res &lt;- wilcox.test(x, y, paired = TRUE, 
                   alternative = &quot;greater&quot;)</code></pre>
<pre><code>## Warning in wilcox.test.default(x, y, paired = TRUE, alternative =
## &quot;greater&quot;): cannot compute exact p-value with ties</code></pre>
<pre><code>## Warning in wilcox.test.default(x, y, paired = TRUE, alternative =
## &quot;greater&quot;): cannot compute exact p-value with zeroes</code></pre>
</div>
<div id="investigating-the-warnings" class="section level2">
<h2>Investigating the warnings</h2>
<p>Before getting to the results, we should investigate the two warnings that resulted from performing the test.</p>
<div id="warning-1-ties" class="section level4">
<h4>Warning 1: ties</h4>
<p>The first warning arises because the test ranks differences in the <code>extra</code> values of pairs. If two pairs share the same difference, ties arise during ranking. We can verify this by computing the difference between the pairs</p>
<pre class="r"><code>x - y</code></pre>
<pre><code>##  [1] 1.2 2.4 1.3 1.3 0.0 1.0 1.8 0.8 4.6 1.4</code></pre>
<p>and finding that pairs 3 and 4 both share the same difference of 1.3. Why are ties a problem? The rank assigned to ties is based on the average of the ranks they span. Thus, if there are many ties, this reduces the expressiveness of the test statistic rendering the Wilcoxon test inappropriate. Since we only have a single tie here, this is not a problem.</p>
</div>
<div id="warning-2-zero-values" class="section level4">
<h4>Warning 2: zero values</h4>
<p>The second warning relates to pairs where the difference is 0. In the sleep data set, this is the case for the pair from the 5th patient (see above). Why are zeros a problem? Remember that the null hypothesis is that the differences of the pairs are centered around 0. However, observing differences where the value is exactly 0 do not give us any information for the rejection of the null. Therefore, these pairs are discarded when computing the test statistic. If this is the case for many of the pairs, the statistical power of the test would drop considerably. Again, this is not a problem for us as only a single zero value is present.</p>
</div>
</div>
<div id="investigating-the-results" class="section level2">
<h2>Investigating the results</h2>
<p>The main result of the test, is its p-value, which can be obtained via:</p>
<pre class="r"><code>res$p.value</code></pre>
<pre><code>## [1] 0.004545349</code></pre>
<p>Since the p-value is less than the 5% significance level, this means we can reject the null hypothesis. Thus, we would be inclined to accept the alternative hypothesis, which states that drug 2 outperforms drug 1.</p>
</div>
<div id="potential-mistakes" class="section level1">
<h1>Potential mistakes</h1>
<p>What kind of mistakes could have happened during this analysis?</p>
<ul>
<li>Using an unpaired test: Given paired data, it is crucial that this information is reflected by the test to ensure statistical power. In case of the sleep data set, pairs are corresponding to measurements for the same patient. In this way, patient-specific effects can be considered. For example, a patient with severe sleep disorder likely receives smaller benefits from treatment than other patients. Even if the benefit is small, we may still see a considerable difference in the effects of the drugs. Thus, the measurements for this patient may still result in a high rank that considerably impacts the test statistic. In an unpaired test, on the other hand, the measurements from such a patient would have a low rank and a correspondingly smaller impact on the test statistic.</li>
<li>Not specifying the appropriate alternative: Remember to always adjust the alternative hypothesis according to the effect that you would like to demonstrate. For example, if I had performed a two-sided test here, a significant result would merely have shown that the drugs perform differently but not which drug seems to perform better, for which a one-tailed test was necessary.</li>
</ul>
</div>
